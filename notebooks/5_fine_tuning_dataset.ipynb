{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ed6fb4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# ðŸ§  Fine-Tuning Explained (Simple and Complete)\n",
    "\n",
    "Fine-tuning means **taking a pre-trained AI model** â€” one that already knows a lot from studying huge amounts of data â€” and **teaching it something more specific** using your own smaller dataset.\n",
    "\n",
    "Imagine a student who already knows English. If you now give them only *medical books*, theyâ€™ll start talking like a doctor. Thatâ€™s what fine-tuning does â€” it **focuses a general model on a specific topic or skill**.\n",
    "\n",
    "When a model is first trained, it learns general knowledge: how sentences are formed, what common words mean, or what basic objects look like. This process is called *pre-training*, and itâ€™s very expensive â€” it needs massive data and huge GPUs.\n",
    "Fine-tuning is the next step where we reuse that knowledge and just make *small adjustments* to the modelâ€™s internal settings (called weights). This way, it becomes better at the new task without forgetting what it already knows.\n",
    "\n",
    "For example:\n",
    "\n",
    "* If you fine-tune ChatGPT on *legal documents*, it becomes better at legal writing.\n",
    "* If you fine-tune a vision model on *medical X-rays*, it becomes good at detecting diseases.\n",
    "* If you fine-tune a voice model on *your accent*, it starts understanding your speech more accurately.\n",
    "\n",
    "Under the hood, fine-tuning works by feeding your dataset into the model and training it again â€” but gently. The model compares its predictions with the correct answers, measures the error (called *loss*), and slightly adjusts its weights to reduce that error. This process repeats many times until the model learns your data patterns.\n",
    "\n",
    "Because large models have billions of parameters, we often donâ€™t train the whole thing again. Instead, we use **efficient fine-tuning methods** like:\n",
    "\n",
    "* **Partial fine-tuning** â€“ only some layers are updated.\n",
    "* **LoRA (Low-Rank Adaptation)** â€“ small adapter layers are added, making fine-tuning faster and cheaper.\n",
    "* **Prompt or prefix tuning** â€“ instead of changing the model, we train a few special tokens that guide it toward your task.\n",
    "\n",
    "Fine-tuning can be used for all types of AI:\n",
    "\n",
    "* In **language models**, it helps write, chat, or summarize in specific domains.\n",
    "* In **vision models**, it helps recognize custom objects.\n",
    "* In **audio models**, it helps adapt to specific voices or sounds.\n",
    "\n",
    "In short, fine-tuning:\n",
    "\n",
    "1. Reuses an existing pre-trained model.\n",
    "2. Trains it on your smaller dataset.\n",
    "3. Slightly updates its knowledge to fit your domain.\n",
    "4. Produces a specialized, smarter version of the model.\n",
    "\n",
    "Itâ€™s powerful because it saves time, data, and money â€” you donâ€™t start from zero; you build on top of something already intelligent.\n",
    "\n",
    "You can think of it like this:\n",
    "\n",
    "> Pre-training builds a brain.\n",
    "> Fine-tuning gives it a profession.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e214a250",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/private/fine_tuning.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/private/fine_tuning.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     lines\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mlen\u001b[39m(lines)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/private/fine_tuning.txt'"
     ]
    }
   ],
   "source": [
    "file_path=\"../data/private/fine_tuning.txt\"\n",
    "with open (file_path,'r',encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8600ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "encryption_message = \"Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.\"\n",
    "media_pattern = \"<Media omitted>\"\n",
    "email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'\n",
    "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "edited_message = \"<This message was edited>\"\n",
    "deleted_message = \"You deleted this message\"\n",
    "null_message = \"null\"\n",
    "created_group_message = \"created group\"\n",
    "added_you_to_group_message = \"added you\"\n",
    "tagging_pattern = r'@[\\w]+'\n",
    "\n",
    "\n",
    "filtered_lines = []\n",
    "for line in lines:\n",
    "    if (\n",
    "            encryption_message not in line and\n",
    "            deleted_message not in line and\n",
    "            null_message != line.split(\" \")[-1] and\n",
    "            media_pattern not in line and\n",
    "            created_group_message not in line and\n",
    "            added_you_to_group_message not in line and\n",
    "            not re.search(email_pattern, line) and\n",
    "            not re.search(url_pattern, line)\n",
    "    ):\n",
    "        line = line.replace(edited_message, \"\").strip()\n",
    "        line = re.sub(tagging_pattern, \"\", line).strip()\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "pattern = r'(\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2}) - (.*?): (.*?)(?=\\n\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} -|$)'\n",
    "content = '\\n'.join(filtered_lines)\n",
    "messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "lines_removed = len(lines) - len(filtered_lines)\n",
    "print(f\"Lines removed: {lines_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d9a10",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100557f6",
   "metadata": {},
   "source": [
    "### 1. Group messages by sender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b120f",
   "metadata": {},
   "source": [
    "If a conversation is structured as follows:  \n",
    "\n",
    "```\n",
    "User 1: Hey!  \n",
    "User 1: How are you?  \n",
    "User 2: I am fine  \n",
    "User 2: And you?  \n",
    "User 1: Good.  \n",
    "```\n",
    "\n",
    "We want to transform it into:  \n",
    "\n",
    "```\n",
    "User 1: Hey!\\nHow are you? \n",
    "User 2: I am fine\\nAnd you?  \n",
    "User 1: Good  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27364da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_messages=[]\n",
    "for _,sender,message in messages:\n",
    "    if grouped_messages and grouped_messages[-1]['sender'] == sender:\n",
    "        grouped_messages[-1][\"message\"] += \"\\n\" + message\n",
    "    else:\n",
    "        grouped_messages.append({\n",
    "            \"sender\":sender,\n",
    "            \"message\": message\n",
    "        })\n",
    "\n",
    "len(grouped_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a0421",
   "metadata": {},
   "source": [
    "### 2. Include special tokens\n",
    "\n",
    "Each message follows this format:\n",
    "```\n",
    "<|startoftext|>Sender<|seprator|>Message<|endoftext|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define special tokens\n",
    "start_of_text_tokens = \"<|startoftext|>\"\n",
    "end_of_text_token = \"<|endoftext|>\"\n",
    "seprator_token=\"<|seprator|>\"\n",
    "\n",
    "fine_tuning_data = []\n",
    "\n",
    "for message in grouped_messages:\n",
    "    sender=message['sender']\n",
    "    message_text = message[\"message\"]\n",
    "    input_sequence= f\"{start_of_text_tokens}{sender}{seprator_token}{message_text}{end_of_text_token}\"\n",
    "    fine_tuning_data.append(input_sequence)\n",
    "\n",
    "len(fine_tuning_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c757f27",
   "metadata": {},
   "source": [
    "### 3. Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path=\"../output/fine_tuning/data/fine_tuning.json\"\n",
    "with open(save_path,\"w\",encoding='utf-8') as f:\n",
    "    json.dump(fine_tuning_data,f,ensure_ascii=False,indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
