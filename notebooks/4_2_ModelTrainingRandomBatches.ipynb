{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/my_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: BasicTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e23c986dd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.529418 M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size = 512\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "912"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../output/combined_text.txt\", \"r\") as f:\n",
    "    text_sequence = f.read()\n",
    "\n",
    "encoded_text_sequence = tokenizer.encode(text_sequence)\n",
    "len(encoded_text_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoded_text_sequence, dtype=torch.long)\n",
    "split_index = int(0.9*len(data))\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 📄 Function: `get_batch(split: str)`\n",
    "\n",
    "This function creates a small batch of input and target sequences for training or validation. It returns two tensors: `x` (inputs) and `y` (targets), both ready to be fed into a model.\n",
    "\n",
    "### 🔧 Code Breakdown\n",
    "\n",
    "```python\n",
    "from typing import Tuple\n",
    "```\n",
    "\n",
    "* This line allows us to specify that the function will return a **tuple** of two PyTorch tensors.\n",
    "\n",
    "```python\n",
    "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "```\n",
    "\n",
    "* Defines the function `get_batch` that takes a string `split`, which should be either `'train'` or `'val'`.\n",
    "* It returns two PyTorch tensors: the input batch `x` and the target batch `y`.\n",
    "\n",
    "```python\n",
    "    data = train_data if split == 'train' else val_data\n",
    "```\n",
    "\n",
    "* Depending on the `split`, it selects the training or validation data.\n",
    "\n",
    "```python\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "```\n",
    "\n",
    "* Randomly selects `batch_size` starting positions from the data to generate sequences.\n",
    "* `block_size` is the length of each input sequence.\n",
    "\n",
    "```python\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "```\n",
    "\n",
    "* For each index, extract a sequence of length `block_size` from the data.\n",
    "* These sequences are stacked into a batch `x`.\n",
    "\n",
    "```python\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
    "```\n",
    "\n",
    "* Creates the target batch `y`, which is the same as `x` but shifted one position forward. This is typically used in language models to predict the next token.\n",
    "\n",
    "```python\n",
    "    x, y = x.to(device), y.to(device)\n",
    "```\n",
    "\n",
    "* Moves the tensors to the desired computing device (CPU or GPU).\n",
    "\n",
    "```python\n",
    "    return x, y\n",
    "```\n",
    "\n",
    "* Returns the input and target batches.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Summary\n",
    "\n",
    "* The function creates batches of sequences for model training or evaluation.\n",
    "* Each input `x` is a chunk of tokens, and the corresponding `y` is the same chunk shifted one token ahead.\n",
    "* It's commonly used for training autoregressive models like GPT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 512]), torch.Size([128, 512]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch('train')\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 🧪 Using the `get_batch` Function\n",
    "\n",
    "```python\n",
    "x, y = get_batch('train')\n",
    "x.shape, y.shape\n",
    "```\n",
    "\n",
    "### 🔍 What This Does:\n",
    "\n",
    "* Calls the `get_batch` function with `'train'` to get a **training batch**.\n",
    "* `x` is the **input tensor**.\n",
    "* `y` is the **target tensor**, which is the input shifted one position ahead.\n",
    "* `.shape` returns the shape (dimensions) of the tensors.\n",
    "\n",
    "### 📐 Expected Output Shape:\n",
    "\n",
    "Assuming the following variables are defined:\n",
    "\n",
    "```python\n",
    "batch_size = 32        # number of sequences in one batch\n",
    "block_size = 128       # length of each sequence\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "x.shape == (32, 128)\n",
    "y.shape == (32, 128)\n",
    "```\n",
    "\n",
    "Each batch contains:\n",
    "\n",
    "* `32` sequences\n",
    "* Each of length `128` tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "eval_iters = 200\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss() -> Dict:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        output[split] = losses.mean()\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 📄 Function: `estimate_loss()`\n",
    "\n",
    "This function estimates the **average loss** of the model on both the **training** and **validation** datasets. It's typically used to monitor how well the model is learning.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Code Explanation\n",
    "\n",
    "```python\n",
    "from typing import Dict\n",
    "```\n",
    "\n",
    "* This allows the function to specify that it returns a dictionary (i.e. `Dict`).\n",
    "\n",
    "```python\n",
    "eval_iters = 200\n",
    "```\n",
    "\n",
    "* The number of iterations (batches) used to estimate the average loss for each split (`train` and `val`).\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "```\n",
    "\n",
    "* A decorator that tells PyTorch **not to compute gradients**. This saves memory and speeds up the evaluation process, since we're not training here—just measuring performance.\n",
    "\n",
    "```python\n",
    "def estimate_loss() -> Dict:\n",
    "```\n",
    "\n",
    "* Defines the function `estimate_loss`, which returns a dictionary with average losses for `'train'` and `'val'`.\n",
    "\n",
    "```python\n",
    "    output = {}\n",
    "```\n",
    "\n",
    "* An empty dictionary to store the average losses.\n",
    "\n",
    "```python\n",
    "    model.eval()\n",
    "```\n",
    "\n",
    "* Sets the model to **evaluation mode**, which turns off dropout and other training-specific behaviors.\n",
    "\n",
    "```python\n",
    "    for split in ['train', 'val']:\n",
    "```\n",
    "\n",
    "* Loops over the two data splits: `'train'` and `'val'`.\n",
    "\n",
    "```python\n",
    "        losses = torch.zeros(eval_iters)\n",
    "```\n",
    "\n",
    "* Creates a tensor to store the loss values for each of the `eval_iters` iterations.\n",
    "\n",
    "```python\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "```\n",
    "\n",
    "* For each iteration:\n",
    "\n",
    "  * Gets a batch of input (`x`) and target (`y`) data.\n",
    "  * Feeds them into the model.\n",
    "  * Extracts the loss and stores it in the `losses` tensor.\n",
    "\n",
    "```python\n",
    "        output[split] = losses.mean()\n",
    "```\n",
    "\n",
    "* Calculates the average loss for the split and stores it in the `output` dictionary.\n",
    "\n",
    "```python\n",
    "    model.train()\n",
    "```\n",
    "\n",
    "* Puts the model back into **training mode**, so it's ready to continue training after the evaluation.\n",
    "\n",
    "```python\n",
    "    return output\n",
    "```\n",
    "\n",
    "* Returns the dictionary with average training and validation losses.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Example Output\n",
    "\n",
    "After calling `estimate_loss()`, you might get something like:\n",
    "\n",
    "```python\n",
    "{'train': 1.923, 'val': 2.140}\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "* The model's average training loss is `1.923`.\n",
    "* The model's average validation loss is `2.140`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What It Does\n",
    "\n",
    "- Disables gradient tracking with `@torch.no_grad()` for efficient evaluation.\n",
    "- Loops through `eval_iters` times to compute average loss on both `train` and `val` datasets.\n",
    "- Switches the model to **evaluation mode** during loss estimation and back to **training mode** afterward.\n",
    "- Returns a dictionary like: `{'train': 1.923, 'val': 2.140}`\n",
    "\n",
    "### 📈 Purpose\n",
    "\n",
    "Useful for monitoring how well the model is learning, without affecting its gradients or training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 💾 Function: `save_checkpoint(...)`\n",
    "\n",
    "This function **saves the model's state** and other important training info (like the optimizer and loss) into a file. It's used to **resume training later** or **keep a backup** of a trained model.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Code Explanation\n",
    "\n",
    "```python\n",
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "```\n",
    "\n",
    "* Defines the `save_checkpoint` function.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `model`: The GPT model to save.\n",
    "  * `optimizer`: The optimizer used during training (e.g. Adam).\n",
    "  * `epoch`: The current epoch number (for resuming training).\n",
    "  * `loss`: The latest loss value (for reference).\n",
    "  * `file_path`: The file name to save the checkpoint (default: `checkpoint.pth`).\n",
    "\n",
    "```python\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "```\n",
    "\n",
    "* Creates a dictionary called `checkpoint` containing:\n",
    "\n",
    "  * The current epoch number\n",
    "  * The model's state (weights and parameters)\n",
    "  * The optimizer's state (learning rates, momentum, etc.)\n",
    "  * The current loss value\n",
    "\n",
    "```python\n",
    "    torch.save(checkpoint, file_path)\n",
    "```\n",
    "\n",
    "* Uses PyTorch’s `torch.save` function to write the `checkpoint` to disk.\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 What This Is Used For\n",
    "\n",
    "Saving a checkpoint allows you to:\n",
    "\n",
    "* Resume training from where you left off.\n",
    "* Avoid losing progress due to crashes or interruptions.\n",
    "* Load the model later for evaluation or inference.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InductorError",
     "evalue": "RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInductorError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m iteration \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m         )\n\u001b[0;32m     18\u001b[0m         train_losses\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m     13\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 14\u001b[0m     _, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     16\u001b[0m output[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:663\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mremove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:760\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[1;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe())\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[0;32m    761\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[0;32m    762\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     TritonBundler\u001b[38;5;241m.\u001b[39mend_compile()\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:745\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[1;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m TritonBundler\u001b[38;5;241m.\u001b[39mbegin_compile()\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     mb_compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    749\u001b[0m     mb_compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1295\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx_subproc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SubprocessFxCompile\n\u001b[0;32m   1293\u001b[0m     scheme \u001b[38;5;241m=\u001b[39m _SubprocessFxCompile()\n\u001b[1;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1197\u001b[0m, in \u001b[0;36m_InProcessFxCompile.codegen_and_compile\u001b[1;34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             compiled_fn \u001b[38;5;241m=\u001b[39m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1185\u001b[0m                 graph,\n\u001b[0;32m   1186\u001b[0m                 wrapper_code\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1194\u001b[0m                 ],\n\u001b[0;32m   1195\u001b[0m             )\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1197\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m   1199\u001b[0m num_bytes, nodes_num_elem, node_runtimes \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcount_bytes()\n\u001b[0;32m   1200\u001b[0m metrics\u001b[38;5;241m.\u001b[39mnum_bytes_accessed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_bytes\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\graph.py:2083\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModuleType:\n\u001b[0;32m   2077\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[0;32m   2078\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.compile_to_module\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2079\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2080\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2081\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2082\u001b[0m     ):\n\u001b[1;32m-> 2083\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\graph.py:2091\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m   2088\u001b[0m \u001b[38;5;66;03m# Currently, if we're here, we don't have to worry about the kernel code, which\u001b[39;00m\n\u001b[0;32m   2089\u001b[0m \u001b[38;5;66;03m# is only available in AOTInductor mode.\u001b[39;00m\n\u001b[0;32m   2090\u001b[0m wrapper_code, _ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 2091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m )\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtriton\u001b[38;5;241m.\u001b[39mautotune_at_compile_time:\n\u001b[0;32m   2094\u001b[0m     tuning_code \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2095\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2096\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile-time auto-tuning block: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2099\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2100\u001b[0m     )\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\graph.py:2002\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1999\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mpush_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 2002\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2004\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m   2005\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2006\u001b[0m     V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mall_codegen_kernel_names,\n\u001b[0;32m   2007\u001b[0m )\n\u001b[0;32m   2008\u001b[0m \u001b[38;5;66;03m# Dump provenance artifacts for debugging trace\u001b[39;00m\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:4135\u001b[0m, in \u001b[0;36mScheduler.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcodegen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler.codegen\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   4132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   4133\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_codegen_partitions()\n\u001b[0;32m   4134\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_inductor\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgraph_partition\n\u001b[1;32m-> 4135\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4136\u001b[0m         )\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:4264\u001b[0m, in \u001b[0;36mScheduler._codegen\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   4262\u001b[0m     backend\u001b[38;5;241m.\u001b[39mcodegen_combo_kernel(node)\n\u001b[0;32m   4263\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[1;32m-> 4264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4266\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4982\u001b[0m, in \u001b[0;36mCppScheduling.codegen_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m   4979\u001b[0m kernel_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_group\n\u001b[0;32m   4981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, OuterLoopFusedSchedulerNode):\n\u001b[1;32m-> 4982\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_outer_loop_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4983\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4984\u001b[0m     nodes: \u001b[38;5;28mlist\u001b[39m[SchedulerNode] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mget_nodes()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4956\u001b[0m, in \u001b[0;36mCppScheduling.codegen_outer_loop_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m   4949\u001b[0m         kernel_group\u001b[38;5;241m.\u001b[39mfinalize_kernel(\n\u001b[0;32m   4950\u001b[0m             outer_fusion_cpp_kernel_proxy,\n\u001b[0;32m   4951\u001b[0m             [\u001b[38;5;241m*\u001b[39mitertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(nodes_list)],\n\u001b[0;32m   4952\u001b[0m         )\n\u001b[0;32m   4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 4956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtry_outer_loop_fusion_with_local_buf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   4957\u001b[0m     \u001b[38;5;66;03m# Reset generated_cpp_vec_kernel_count to codegen again\u001b[39;00m\n\u001b[0;32m   4958\u001b[0m     metrics\u001b[38;5;241m.\u001b[39mgenerated_cpp_vec_kernel_count \u001b[38;5;241m=\u001b[39m generated_cpp_vec_kernel_count\n\u001b[0;32m   4959\u001b[0m     cpp_kernel_proxy_list\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4927\u001b[0m, in \u001b[0;36mCppScheduling.codegen_outer_loop_node.<locals>.try_outer_loop_fusion_with_local_buf\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m   4925\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _node \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mget_outer_nodes():\n\u001b[0;32m   4926\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_node, (FusedSchedulerNode, SchedulerNode))\n\u001b[1;32m-> 4927\u001b[0m     cpp_kernel_proxy \u001b[38;5;241m=\u001b[39m \u001b[43mCppKernelProxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4928\u001b[0m     cpp_kernel_proxy\u001b[38;5;241m.\u001b[39mcodegen_nodes(_node\u001b[38;5;241m.\u001b[39mget_nodes())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   4929\u001b[0m     cpp_kernel_proxy_list\u001b[38;5;241m.\u001b[39mappend(cpp_kernel_proxy)\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3734\u001b[0m, in \u001b[0;36mCppKernelProxy.__init__\u001b[1;34m(self, kernel_group)\u001b[0m\n\u001b[0;32m   3732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop_nest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_ranges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpicked_vec_isa: cpu_vec_isa\u001b[38;5;241m.\u001b[39mVecISA \u001b[38;5;241m=\u001b[39m \u001b[43mcpu_vec_isa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpick_vec_isa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels: \u001b[38;5;28mlist\u001b[39m[CppKernel] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:418\u001b[0m, in \u001b[0;36mpick_vec_isa\u001b[1;34m()\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_fbcode() \u001b[38;5;129;01mand\u001b[39;00m (platform\u001b[38;5;241m.\u001b[39mmachine() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx86_64\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMD64\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VecAVX2()\n\u001b[1;32m--> 418\u001b[0m _valid_vec_isa_list: \u001b[38;5;28mlist\u001b[39m[VecISA] \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_vec_isa_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _valid_vec_isa_list:\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_vec_isa\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:405\u001b[0m, in \u001b[0;36mvalid_vec_isa_list\u001b[1;34m()\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     _cpu_supported_x86_isa \u001b[38;5;241m=\u001b[39m x86_isa_checker()\n\u001b[1;32m--> 405\u001b[0m     \u001b[43misa_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43misa\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_vec_isa_list\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_cpu_supported_x86_isa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43misa\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:408\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     _cpu_supported_x86_isa \u001b[38;5;241m=\u001b[39m x86_isa_checker()\n\u001b[0;32m    405\u001b[0m     isa_list\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m    406\u001b[0m         isa\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m isa \u001b[38;5;129;01min\u001b[39;00m supported_vec_isa_list\n\u001b[1;32m--> 408\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(flag \u001b[38;5;129;01min\u001b[39;00m _cpu_supported_x86_isa \u001b[38;5;28;01mfor\u001b[39;00m flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(isa)\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;129;01mand\u001b[39;00m isa\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:142\u001b[0m, in \u001b[0;36mVecISA.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__bool__impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvec_isa_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:152\u001b[0m, in \u001b[0;36mVecISA.__bool__impl\u001b[1;34m(self, vec_isa_ok)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_fbcode():\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVecISA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_avx_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:102\u001b[0m, in \u001b[0;36mVecISA.check_build\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lock_dir, LOCK_TIMEOUT, write\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m     CppBuilder,\n\u001b[0;32m     95\u001b[0m     CppTorchOptions,\n\u001b[0;32m     96\u001b[0m     normalize_path_separator,\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     99\u001b[0m key, input_path \u001b[38;5;241m=\u001b[39m write(\n\u001b[0;32m    100\u001b[0m     code,\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 102\u001b[0m     extra\u001b[38;5;241m=\u001b[39m\u001b[43m_get_isa_dry_compile_fingerprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arch_flags\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_filelock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileLock\n\u001b[0;32m    106\u001b[0m lock_dir \u001b[38;5;241m=\u001b[39m get_lock_dir()\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:28\u001b[0m, in \u001b[0;36m_get_isa_dry_compile_fingerprint\u001b[1;34m(isa_flags)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_isa_dry_compile_fingerprint\u001b[39m(isa_flags: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# ISA dry compile will cost about 1 sec time each startup time.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Please check the issue: https://github.com/pytorch/pytorch/issues/100378\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# and generated them to output binary hash path.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# It would optimize and skip compile existing binary.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_compiler_version_info, get_cpp_compiler\n\u001b[1;32m---> 28\u001b[0m     compiler_info \u001b[38;5;241m=\u001b[39m get_compiler_version_info(\u001b[43mget_cpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     29\u001b[0m     torch_version \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m     30\u001b[0m     fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misa_flags\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:148\u001b[0m, in \u001b[0;36mget_cpp_compiler\u001b[1;34m()\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_WINDOWS:\n\u001b[0;32m    147\u001b[0m     compiler \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCXX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 148\u001b[0m     \u001b[43mcheck_compiler_exist_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_fbcode():\n",
      "File \u001b[1;32mf:\\PRGRAMME\\PYTHON 3.12\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:139\u001b[0m, in \u001b[0;36mcheck_compiler_exist_windows\u001b[1;34m(compiler)\u001b[0m\n\u001b[0;32m    137\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mcheck_output([compiler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/help\u001b[39m\u001b[38;5;124m\"\u001b[39m], stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompiler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mSubprocessError:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# Expected that some compiler(clang, clang++) is exist, but they not support `/help` args.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mInductorError\u001b[0m: RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-4\n",
    "save_interval = 100\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iteration}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iteration % save_interval == 0:\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=iteration,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_1/checkpoint_{iteration}.pth\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Training Loop (Line-by-Line Explanation)\n",
    "\n",
    "This code trains a language model using the AdamW optimizer. It evaluates the loss at regular intervals, saves the model periodically, and collects training/validation losses for monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Setup\n",
    "\n",
    "```python\n",
    "max_iters = 1000                # Total number of training iterations (batches)\n",
    "eval_interval = 10              # How often (in iterations) to evaluate and print loss\n",
    "learning_rate = 1e-4            # Learning rate for the optimizer\n",
    "save_interval = 100             # How often (in iterations) to save the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # Initialize the optimizer\n",
    "```\n",
    "\n",
    "- We train the model for 1000 steps.\n",
    "- Every 10 steps, we evaluate how well the model is doing.\n",
    "- Every 100 steps, we save the model checkpoint.\n",
    "- We use the `AdamW` optimizer, which is good for training transformers.\n",
    "\n",
    "```python\n",
    "train_losses = []   # List to store training loss over time\n",
    "val_losses = []     # List to store validation loss over time\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Training Loop\n",
    "\n",
    "```python\n",
    "for iteration in range(max_iters):   # Loop over training steps\n",
    "```\n",
    "\n",
    "- Loop from `0` to `999` (total 1000 iterations).\n",
    "\n",
    "```python\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "```\n",
    "\n",
    "- Every `eval_interval` steps (e.g. every 10 steps), or on the final step:\n",
    "- We check how the model is performing.\n",
    "\n",
    "```python\n",
    "        losses = estimate_loss()  # Estimate the average train and val loss\n",
    "```\n",
    "\n",
    "- Calls the `estimate_loss()` function to evaluate how well the model is doing on both training and validation data.\n",
    "\n",
    "```python\n",
    "        print(\n",
    "            f\"step {iteration}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- Prints the current step and the train/validation loss (rounded to 4 decimal places).\n",
    "\n",
    "```python\n",
    "        train_losses.append(losses['train'])  # Save the train loss to list\n",
    "        val_losses.append(losses['val'])      # Save the val loss to list\n",
    "```\n",
    "\n",
    "- Appends the losses to their respective lists so you can track them later (e.g. plot a graph).\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Model Training Step\n",
    "\n",
    "```python\n",
    "    x_batch, y_batch = get_batch('train')  # Get a batch of training data\n",
    "```\n",
    "\n",
    "- Retrieves a random batch of input/output pairs for training.\n",
    "\n",
    "```python\n",
    "    logits, loss = model(x_batch, y_batch)  # Forward pass: predict and calculate loss\n",
    "```\n",
    "\n",
    "- Feeds the input batch through the model.\n",
    "- Gets predictions (`logits`) and calculates the loss.\n",
    "\n",
    "```python\n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear previous gradients\n",
    "```\n",
    "\n",
    "- Clears old gradients before computing new ones.\n",
    "\n",
    "```python\n",
    "    loss.backward()  # Backward pass: compute gradients\n",
    "```\n",
    "\n",
    "- Calculates gradients by backpropagation.\n",
    "\n",
    "```python\n",
    "    optimizer.step()  # Update model weights\n",
    "```\n",
    "\n",
    "- Applies the gradients to update the model parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 💾 Save Checkpoint\n",
    "\n",
    "```python\n",
    "    if iteration % save_interval == 0:\n",
    "```\n",
    "\n",
    "- Every `save_interval` steps (e.g. every 100 steps):\n",
    "\n",
    "```python\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=iteration,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_1/checkpoint_{iteration}.pth\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- Saves the model, optimizer state, current step (`iteration`), and current loss to a checkpoint file.\n",
    "- File is named like `checkpoint_0.pth`, `checkpoint_100.pth`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "\n",
    "This training loop does the following:\n",
    "- Trains a language model for 1000 iterations.\n",
    "- Evaluates and prints loss every 10 steps.\n",
    "- Saves the model every 100 steps.\n",
    "- Tracks loss history for plotting or analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAV1BJREFUeJzt3XlYVdXi//HPYQYRMFTANHFAAUUsTcIG9YaCmomVIpmKmeUtUy/mLSccy8osTTOzUrMbaZra5ISkZUqaU2oOlTll4hjhCAj794c/zrcTqBzdeETfr+c5T5y11957LfaS+LD2XsdiGIYhAAAAAMBVcXJ0AwAAAADgRkC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCcNNLSkpScHDwFe07YsQIWSwWcxt0ndm7d68sFotmzpx5zc9tsVg0YsQI6/uZM2fKYrFo7969l903ODhYSUlJprbnasYKyraVK1fKYrFo5cqVjm4KgOsY4QrAdctisZToxS87jte3b19ZLBb9+uuvF60zZMgQWSwWbdmy5Rq2zH5//PGHRowYoc2bNzu6KVaFAfe1115zdFNKZP/+/erdu7eCg4Pl7u6uypUrKz4+XqtXr3Z002wkJSWV6GeM2SEdwI3LxdENAICL+fDDD23ez5o1S2lpaUXKw8LCruo87777rgoKCq5o36FDh+qFF164qvPfCLp06aJJkyYpNTVVKSkpxdb5+OOPFRERoQYNGlzxebp27arOnTvL3d39io9xOX/88YdGjhyp4OBgNWzY0Gbb1YyVm8Xq1avVpk0bSdITTzyh8PBwZWZmaubMmbr33ns1ceJEPfvssw5u5QVPPfWUYmJirO/37NmjlJQUPfnkk7r33nut5bVq1VJUVJTOnj0rNzc3RzQVQBlBuAJw3Xrsscds3n///fdKS0srUv5PZ86ckZeXV4nP4+rqekXtkyQXFxe5uPCjNCoqSrVr19bHH39cbLjKyMjQnj179PLLL1/VeZydneXs7HxVx7gaVzNWbgZ//vmnHnnkEXl6emr16tWqVauWdVtycrJiY2PVv39/NWrUSE2bNr1m7Tp37pzc3Nzk5GR7w050dLSio6Ot79evX6+UlBRFR0cX+3PGw8Oj1NsKoGzjtkAAZVrz5s1Vv359bdiwQffdd5+8vLw0ePBgSdJnn32mtm3bqkqVKnJ3d1etWrU0evRo5efn2xzjn8/R/P0WrGnTpqlWrVpyd3fXnXfeqR9++MFm3+KeubJYLOrTp48WLlyo+vXry93dXfXq1dOSJUuKtH/lypVq3LixPDw8VKtWLb3zzjslfo5r1apV6tixo2677Ta5u7urWrVq+s9//qOzZ88W6Z+3t7cOHjyo+Ph4eXt7q1KlSnruueeKfC+ysrKUlJQkX19f+fn5qXv37srKyrpsW6QLs1c7d+7Uxo0bi2xLTU2VxWJRYmKicnNzlZKSokaNGsnX11flypXTvffeqxUrVlz2HMU9c2UYhsaMGaOqVavKy8tLLVq00E8//VRk3xMnTui5555TRESEvL295ePjo9atW+vHH3+01lm5cqXuvPNOSVKPHj2st4UVPm9W3DNXp0+f1oABA1StWjW5u7urbt26eu2112QYhk09e8bFlTpy5Ih69uypgIAAeXh4KDIyUh988EGRerNnz1ajRo1Uvnx5+fj4KCIiQhMnTrRuz8vL08iRIxUSEiIPDw/5+/vrnnvuUVpa2iXP/8477ygzM1Pjxo2zCVaS5OnpqQ8++EAWi0WjRo2SdCHMWCyWYtu4dOlSWSwWffnll9aygwcP6vHHH1dAQID1+zd9+nSb/QqfjZo9e7aGDh2qW2+9VV5eXsrOzr78N/ASinvmqvDnz5YtW9SsWTN5eXmpdu3amjdvniTpm2++UVRUlDw9PVW3bl0tX768yHFL0icAZQd/bgVQ5h0/flytW7dW586d9dhjjykgIEDShV/Evb29lZycLG9vb3399ddKSUlRdna2xo0bd9njpqam6uTJk3rqqadksVj06quv6qGHHtJvv/122RmM7777TvPnz9fTTz+t8uXL680339TDDz+s/fv3y9/fX5K0adMmxcXFKSgoSCNHjlR+fr5GjRqlSpUqlajfc+fO1ZkzZ/Tvf/9b/v7+WrdunSZNmqTff/9dc+fOtambn5+v2NhYRUVF6bXXXtPy5cs1fvx41apVS//+978lXQgp7du313fffafevXsrLCxMCxYsUPfu3UvUni5dumjkyJFKTU3VHXfcYXPuTz75RPfee69uu+02HTt2TO+9954SExPVq1cvnTx5Uu+//75iY2O1bt26IrfiXU5KSorGjBmjNm3aqE2bNtq4caNatWql3Nxcm3q//fabFi5cqI4dO6pGjRo6fPiw3nnnHTVr1kzbt29XlSpVFBYWplGjRhW5NexisyyGYejBBx/UihUr1LNnTzVs2FBLly7VwIEDdfDgQb3xxhs29UsyLq7U2bNn1bx5c/3666/q06ePatSooblz5yopKUlZWVnq16+fJCktLU2JiYm6//779corr0iSduzYodWrV1vrjBgxQmPHjtUTTzyhJk2aKDs7W+vXr9fGjRvVsmXLi7bhiy++kIeHhzp16lTs9ho1auiee+7R119/rbNnz6px48aqWbOmPvnkkyLjbM6cOapQoYJiY2MlSYcPH9Zdd91lDamVKlXS4sWL1bNnT2VnZ6t///42+48ePVpubm567rnnlJOTU2q38/3555964IEH1LlzZ3Xs2FFvv/22OnfurI8++kj9+/dX79699eijj2rcuHF65JFHdODAAZUvX/6K+gSgDDAAoIx45plnjH/+2GrWrJkhyZg6dWqR+mfOnClS9tRTTxleXl7GuXPnrGXdu3c3qlevbn2/Z88eQ5Lh7+9vnDhxwlr+2WefGZKML774wlo2fPjwIm2SZLi5uRm//vqrtezHH380JBmTJk2ylrVr187w8vIyDh48aC375ZdfDBcXlyLHLE5x/Rs7dqxhsViMffv22fRPkjFq1CiburfffrvRqFEj6/uFCxcakoxXX33VWnb+/Hnj3nvvNSQZM2bMuGyb7rzzTqNq1apGfn6+tWzJkiWGJOOdd96xHjMnJ8dmvz///NMICAgwHn/8cZtyScbw4cOt72fMmGFIMvbs2WMYhmEcOXLEcHNzM9q2bWsUFBRY6w0ePNiQZHTv3t1adu7cOZt2GcaFa+3u7m7zvfnhhx8u2t9/jpXC79mYMWNs6j3yyCOGxWKxGQMlHRfFKRyT48aNu2idCRMmGJKM//3vf9ay3NxcIzo62vD29jays7MNwzCMfv36GT4+Psb58+cveqzIyEijbdu2l2xTcfz8/IzIyMhL1unbt68hydiyZYthGIYxaNAgw9XV1ebfWk5OjuHn52czHnr27GkEBQUZx44dszle586dDV9fX+u/hxUrVhiSjJo1axb7b+RSLnXtC4+7YsUKa1nhz5/U1FRr2c6dOw1JhpOTk/H9999by5cuXVrk2CXtE4Cyg9sCAZR57u7u6tGjR5FyT09P69cnT57UsWPHdO+99+rMmTPauXPnZY+bkJCgChUqWN8XzmL89ttvl903JibG5raoBg0ayMfHx7pvfn6+li9frvj4eFWpUsVar3bt2mrduvVljy/Z9u/06dM6duyYmjZtKsMwtGnTpiL1e/fubfP+3nvvtenLokWL5OLiYp3Jki4842TP4gOPPfaYfv/9d3377bfWstTUVLm5ualjx47WYxbOIhQUFOjEiRM6f/68GjduXOwthZeyfPly5ebm6tlnn7W5lbK4v/i7u7tbn7nJz8/X8ePH5e3trbp169p93kKLFi2Ss7Oz+vbta1M+YMAAGYahxYsX25RfblxcjUWLFikwMFCJiYnWMldXV/Xt21enTp3SN998I0ny8/PT6dOnL3mLn5+fn3766Sf98ssvdrXh5MmT1lmZiyncXnibXkJCgvLy8jR//nxrnWXLlikrK0sJCQmSLswQfvrpp2rXrp0Mw9CxY8esr9jYWP31119FrmH37t1t/o2UFm9vb3Xu3Nn6vm7duvLz81NYWJiioqKs5YVfF17rK+kTgOsf4QpAmXfrrbcWe8vPTz/9pA4dOsjX11c+Pj6qVKmS9SH1v/7667LHve2222zeFwatP//80+59C/cv3PfIkSM6e/asateuXaRecWXF2b9/v5KSknTLLbdYn6Nq1qyZpKL98/DwKHK74d/bI0n79u1TUFCQvL29berVrVu3RO2RpM6dO8vZ2VmpqamSLiwksGDBArVu3domqH7wwQdq0KCB9XmeSpUq6auvvirRdfm7ffv2SZJCQkJsyitVqmRzPulCkHvjjTcUEhIid3d3VaxYUZUqVdKWLVvsPu/fz1+lSpUigaJwBcvC9hW63Li4Gvv27VNISEiRRRv+2Zann35aderUUevWrVW1alU9/vjjRZ77GjVqlLKyslSnTh1FRERo4MCBJVpCv3z58jp58uQl6xRuL/yeRUZGKjQ0VHPmzLHWmTNnjipWrKh//etfkqSjR48qKytL06ZNU6VKlWxehX9YOXLkiM15atSocdn2mqFq1apFnpH09fVVtWrVipRJ//fz40r6BOD6xzNXAMq84v46nZWVpWbNmsnHx0ejRo1SrVq15OHhoY0bN+r5558v0XLaF1uVzvjHQgVm71sS+fn5atmypU6cOKHnn39eoaGhKleunA4ePKikpKQi/btWK+xVrlxZLVu21Keffqq33npLX3zxhU6ePKkuXbpY6/zvf/9TUlKS4uPjNXDgQFWuXFnOzs4aO3asdu/eXWpte+mllzRs2DA9/vjjGj16tG655RY5OTmpf//+12x59dIeFyVRuXJlbd68WUuXLtXixYu1ePFizZgxQ926dbMuLHHfffdp9+7d+uyzz7Rs2TK99957euONNzR16lQ98cQTFz12WFiYNm3apJycnIsul79lyxa5urraBOKEhAS9+OKLOnbsmMqXL6/PP/9ciYmJ1pU4C6/PY489dtFnAP+5xP+1mLWSLn5NL3etr6RPAK5/hCsAN6SVK1fq+PHjmj9/vu677z5r+Z49exzYqv9TuXJleXh4FPuhu5f6IN5CW7du1c8//6wPPvhA3bp1s5ZfbjW3S6levbrS09N16tQpm9mrXbt22XWcLl26aMmSJVq8eLFSU1Pl4+Ojdu3aWbfPmzdPNWvW1Pz5823+4j98+PArarMk/fLLL6pZs6a1/OjRo0Vmg+bNm6cWLVro/ffftynPyspSxYoVre9LslLj38+/fPnyIrfDFd52Wti+a6F69erasmWLCgoKbGavimuLm5ub2rVrp3bt2qmgoEBPP/203nnnHQ0bNsw6c3rLLbeoR48e6tGjh06dOqX77rtPI0aMuGS4euCBB5SRkaG5c+cWu5T53r17tWrVKsXExNiEn4SEBI0cOVKffvqpAgIClJ2dbXOrXaVKlVS+fHnl5+fbfC5VWXYj9gkAtwUCuEEV/tX47zMCubm5mjJliqOaZMPZ2VkxMTFauHCh/vjjD2v5r7/+WuQ5nYvtL9n2zzAMm+W07dWmTRudP39eb7/9trUsPz9fkyZNsus48fHx8vLy0pQpU7R48WI99NBDNp8PVFzb165dq4yMDLvbHBMTI1dXV02aNMnmeBMmTChS19nZucgM0dy5c3Xw4EGbsnLlyklSiZagb9OmjfLz8zV58mSb8jfeeEMWi6XEz8+ZoU2bNsrMzLS5ve78+fOaNGmSvL29rbeMHj9+3GY/Jycn6wxJTk5OsXW8vb1Vu3Zt6/aLeeqpp1S5cmUNHDiwyHNk586dU48ePWQYRpHPQgsLC1NERITmzJmjOXPmKCgoyOaPIs7Oznr44Yf16aefatu2bUXOe/To0Uu263p0I/YJADNXAG5QTZs2VYUKFdS9e3f17dtXFotFH3744TW9/epyRowYoWXLlunuu+/Wv//9b+sv6fXr19fmzZsvuW9oaKhq1aql5557TgcPHpSPj48+/fTTq3p2p127drr77rv1wgsvaO/evQoPD9f8+fPtfh7J29tb8fHx1ueu/n5LoHRhdmP+/Pnq0KGD2rZtqz179mjq1KkKDw/XqVOn7DpX4ed1jR07Vg888IDatGmjTZs2afHixTazUYXnHTVqlHr06KGmTZtq69at+uijj2xmvCSpVq1a8vPz09SpU1W+fHmVK1dOUVFRxT7D065dO7Vo0UJDhgzR3r17FRkZqWXLlumzzz5T//79i3zW09VKT0/XuXPnipTHx8frySef1DvvvKOkpCRt2LBBwcHBmjdvnlavXq0JEyZYZ9aeeOIJnThxQv/6179UtWpV7du3T5MmTVLDhg2tz2eFh4erefPmatSokW655RatX79e8+bNU58+fS7ZPn9/f82bN09t27bVHXfcoSeeeELh4eHKzMzUzJkz9euvv2rixInFLm2fkJCglJQUeXh4qGfPnkWeHXv55Ze1YsUKRUVFqVevXgoPD9eJEye0ceNGLV++XCdOnLjSb6vD3Ih9Am52hCsANyR/f399+eWXGjBggIYOHaoKFSroscce0/3332/93BxHa9SokRYvXqznnntOw4YNU7Vq1TRq1Cjt2LHjsqsZurq66osvvlDfvn01duxYeXh4qEOHDurTp48iIyOvqD1OTk76/PPP1b9/f/3vf/+TxWLRgw8+qPHjx+v222+361hdunRRamqqgoKCrIsSFEpKSlJmZqbeeecdLV26VOHh4frf//6nuXPn2nxAa0mNGTNGHh4emjp1qvUX1WXLlqlt27Y29QYPHqzTp08rNTVVc+bM0R133KGvvvpKL7zwgk09V1dXffDBBxo0aJB69+6t8+fPa8aMGcWGq8LvWUpKiubMmaMZM2YoODhY48aN04ABA+zuy+UsWbKk2A8dDg4OVv369bVy5Uq98MIL+uCDD5Sdna26detqxowZSkpKstZ97LHHNG3aNE2ZMkVZWVkKDAxUQkKCRowYYQ00ffv21eeff65ly5YpJydH1atX15gxYzRw4MDLtvHee+/Vli1b9NJLL2nu3Lk6dOiQfH191bRpU02fPl333HNPsfslJCRo6NChOnPmjHWVwL8LCAjQunXrNGrUKM2fP19TpkyRv7+/6tWrZ/28rrLmRuwTcLOzGNfTn3EBAIqPj7+iZbABAIBj8cwVADjQ2bNnbd7/8ssvWrRokZo3b+6YBgEAgCvGzBUAOFBQUJCSkpJUs2ZN7du3T2+//bZycnK0adOmIp/dBAAArm88cwUADhQXF6ePP/5YmZmZcnd3V3R0tF566SWCFQAAZRAzVwAAAABgAp65AgAAAAATEK4AAAAAwAQ8c1WMgoIC/fHHHypfvrwsFoujmwMAAADAQQzD0MmTJ1WlSpUiH3D+T4SrYvzxxx+qVq2ao5sBAAAA4Dpx4MABVa1a9ZJ1CFfFKF++vKQL30AfHx8HtwYXk5eXp2XLlqlVq1ZydXV1dHNQBjBmYC/GDOzBeIG9GDNlQ3Z2tqpVq2bNCJdCuCpG4a2APj4+hKvrWF5enry8vOTj48MPJJQIYwb2YszAHowX2IsxU7aU5HEhFrQAAAAAABMQrgAAAADABIQrAAAAADABz1wBAACgTMjPz1deXp6jm2GavLw8ubi46Ny5c8rPz3d0c25azs7OcnFxMeUjmAhXAAAAuO6dOnVKv//+uwzDcHRTTGMYhgIDA3XgwAE+W9XBvLy8FBQUJDc3t6s6DuEKAAAA17X8/Hz9/vvv8vLyUqVKlW6YIFJQUKBTp07J29v7sh9Oi9JhGIZyc3N19OhR7dmzRyEhIVd1LQhXAAAAuK7l5eXJMAxVqlRJnp6ejm6OaQoKCpSbmysPDw/ClQN5enrK1dVV+/bts16PK8VVBAAAQJlwo8xY4fpjVrglXAEAAACACQhXAAAAAGACwhUAAABQRgQHB2vChAmObgYugnAFAAAAmMxisVzyNWLEiCs67g8//KAnn3zyqtrWvHlz9e/f/6qOgeKxWiAAAABgskOHDlm/njNnjlJSUrRr1y5rmbe3t/VrwzB0/vx5ubhc/lfzSpUqmdtQmIqZKwAAAJQphmHoTO55h7xK+iHGgYGB1pevr68sFov1/c6dO1W+fHktXrxYzZs3l6enp7777jvt3r1b7du3V0BAgLy9vXXnnXdq+fLlNsf9522BFotF7733njp06CAvLy+FhITo888/v6rv76effqp69erJ3d1dwcHBGj9+vM32KVOmKCQkRB4eHgoICNAjjzxi3TZv3jxFRETI09NT/v7+iomJ0enTp6+qPWUJM1cAAAAoU87m5Ss8ZalDzr19VKy83Mz5FXrw4MEaMWKE6tevL39/fx04cEBt2rTRiy++KHd3d82aNUvt2rXTrl27dNttt130OCNHjtSrr76qcePGadKkSerSpYv27dunW265xe42bdiwQZ06ddKIESOUkJCgNWvW6Omnn5a/v7+SkpK0fv169e3bVx9++KGaNm2qEydOaNWqVZIuzNYlJibq1VdfVYcOHXTy5EmtWrWqxIH0RkC4AgAAABxgxIgRatGihXx8fOTk5KRbbrlFkZGR1u2jR4/WggUL9Pnnn6tPnz4XPU5SUpISExMlSS+99JLefPNNrVu3TnFxcXa36fXXX9f999+vYcOGSZLq1Kmj7du3a9y4cUpKStL+/ftVrlw5PfDAAypfvryqV6+u22+/XdKFcHX+/Hk99NBDql69uiQpIiLC7jaUZYQrAAAAlCmers7aPirWYec2S+PGjW3enzp1SiNGjNBXX31lDSpnz57V/v37L3mcBg0aWL8uV66cfHx8dOTIkStq044dO9S+fXubsrvvvlsTJkxQfn6+WrZsqerVq6tmzZqKi4tTXFyc9ZbEyMhI3X///YqIiFBsbKxatWqlRx55RBUqVLiitpRFPHMFAACAMsViscjLzcUhL4vFYlo/ypUrZ/P+ueee04IFC/TSSy9p1apV2rx5syIiIpSbm3vJ47i6uhb5/hQUFJjWzr8rX768Nm7cqI8//lhBQUFKSUlRZGSksrKy5OzsrLS0NC1evFjh4eGaNGmS6tatqz179pRKW65HhCsAAADgOrB69WolJSWpQ4cOioiIUGBgoPbu3XtN2xAWFqbVq1cXaVedOnXk7Hxh1s7FxUUxMTF69dVXtWXLFu3du1dff/21pAvB7u6779bIkSO1adMmubm5acGCBde0D47EbYEAAADAdSAkJETz589Xu3btZLFYNGzYsFKbgTp69Kg2b95sUxYUFKQBAwbozjvv1OjRo5WQkKCMjAxNnjxZU6ZMkSR9+eWX+u2333TfffepQoUKWrRokQoKClS3bl2tXbtW6enpatWqlSpXrqy1a9fq6NGjCgsLK5U+XI8IVwAAAMB14PXXX9fjjz+upk2bqmLFinr++eeVnZ1dKudKTU1VamqqTdno0aM1dOhQffLJJ0pJSdHo0aMVFBSkUaNGKSkpSZLk5+en+fPna8SIETp37pxCQkL08ccfq169etqxY4e+/fZbTZgwQdnZ2apevbrGjx+v1q1bl0ofrkcW42ZaG7GEsrOz5evrq7/++ks+Pj6Obg4uIi8vT4sWLVKbNm2K3GsMFIcxA3sxZmAPxkvpOXfunPbs2aMaNWrIw8PD0c0xTUFBgbKzs62rBcJxLjXG7MkGXEUAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATHBdhKu33npLwcHB8vDwUFRUlNatW3fJ+nPnzlVoaKg8PDwUERGhRYsW2WxPSkqSxWKxecXFxZVmFwAAAADc5BwerubMmaPk5GQNHz5cGzduVGRkpGJjY3XkyJFi669Zs0aJiYnq2bOnNm3apPj4eMXHx2vbtm029eLi4nTo0CHr6+OPP74W3QEAAABwk3J4uHr99dfVq1cv9ejRQ+Hh4Zo6daq8vLw0ffr0YutPnDhRcXFxGjhwoMLCwjR69Gjdcccdmjx5sk09d3d3BQYGWl8VKlS4Ft0BAAAAcJNyceTJc3NztWHDBg0aNMha5uTkpJiYGGVkZBS7T0ZGhpKTk23KYmNjtXDhQpuylStXqnLlyqpQoYL+9a9/acyYMfL39y/2mDk5OcrJybG+z87OlnThk9bz8vKupGu4BgqvDdcIJcWYgb0YM7AH46X05OXlyTAMFRQUqKCgwNHNMY1hGNb/Xqxf//rXvxQZGak33nhDklSzZk3169dP/fr1u+hxnZ2d9emnnyo+Pv6q2mfWccqCgoICGYahvLw8OTs722yz59+0Q8PVsWPHlJ+fr4CAAJvygIAA7dy5s9h9MjMzi62fmZlpfR8XF6eHHnpINWrU0O7duzV48GC1bt1aGRkZRb5ZkjR27FiNHDmySPmyZcvk5eV1JV3DNZSWluboJqCMYczAXowZ2IPxYj4XFxcFBgbq1KlTys3NdXRzSqRz5846f/685s2bV2TbmjVr1LZtW61atUr169fXyZMnL3qc8+fPKzc31/rH/+XLl8vLy8v6/mLOnj172TqFXn75ZX311VdatWqVTfnOnTvl5+dX4uNcidTUVA0aNEj79u0rtXOURG5urs6ePatvv/1W58+ft9l25syZEh/HoeGqtHTu3Nn6dUREhBo0aKBatWpp5cqVuv/++4vUHzRokM1sWHZ2tqpVq6ZWrVrJx8fnmrQZ9svLy1NaWppatmwpV1dXRzcHZQBjBvZizMAejJfSc+7cOR04cEDe3t7y8PBwdHNK5Mknn1THjh2VnZ2tqlWr2mybO3euGjdurOjoaJ08eVLly5eXxWIp9jguLi5yc3Oz/k5a0t9NPT09S1zX3d1dzs7ORepfi9+DPTw8ZLFYHP4797lz5+Tp6an77ruvyBizJ1w6NFxVrFhRzs7OOnz4sE354cOHFRgYWOw+gYGBdtWXLkyfVqxYUb/++mux4crd3V3u7u5Fyl1dXfnhWAZwnWAvxgzsxZiBPRgv5svPz5fFYpGTk5OcnJwkw5DySj6bYCpXL+kiQejvHnzwQVWqVEmzZs3S0KFDreWnTp3SvHnzNG7cOJ04cUK9e/fW999/rz///FO1atXS4MGDlZiYaHOswr5LUnBwsPr376/+/ftLkn755Rf17NlT69atU82aNTVx4kRJ+r/vlaTnn39eCxYs0O+//67AwEB16dJFKSkpcnV11cyZMzVq1ChJst7hNWPGDOvq2wsWLLDeFrh161b169dPGRkZ8vLy0sMPP6zXX39d3t7eki6s2J2VlaV77rlH48ePV25urjp37qwJEyZc9N9EYRsL//tP+/fv17PPPqv09HQ5OTkpLi5OkyZNst7J9uOPP6p///5av369LBaLQkJC9M4776hx48bat2+f+vTpo++++065ubkKDg7WuHHj1KZNm2LbYbFYiv33a8+/Z4eGKzc3NzVq1Ejp6enWi1ZQUKD09HT16dOn2H2io6OVnp5uHVDShen36Ojoi57n999/1/HjxxUUFGRm8wEAAOAIeWekl6o45tyD/5Dcyl22mouLi7p166aZM2dqyJAh1pmpuXPnKj8/X4mJicrOzlbDhg01ZMgQ+fn56auvvlLXrl1Vq1YtNWnS5LLnKCgo0EMPPaSAgACtXbtWf/31l83vyIXKly+vmTNnqkqVKtq6dat69eql8uXL67///a8SEhK0bds2LVmyRMuXL5ck+fr6FjnG6dOnFRsbq+joaP3www86cuSInnjiCfXp00czZ8601luxYoWCgoK0YsUK/frrr0pISFDDhg3Vq1evy/anuP61b99e3t7e+uabb3T+/Hk988wzSkhI0MqVKyVJXbp00e233663335bzs7O2rx5szUMPfPMM8rNzdW3336rcuXKafv27dYgWFocfltgcnKyunfvrsaNG6tJkyaaMGGCTp8+rR49ekiSunXrpltvvVVjx46VJPXr10/NmjXT+PHj1bZtW82ePVvr16/XtGnTJF34a8DIkSP18MMPKzAwULt379Z///tf1a5dW7GxsQ7rJwAAAG4ujz/+uMaNG6dvvvlGzZs3l3RhVujhhx+Wr6+vypcvr2effVY+Pj5ycnLSs88+q6VLl+qTTz4pUbhavny5du7cqaVLl6pKlQth86WXXlLr1q1t6v195iw4OFjPPfecZs+erf/+97/y9PSUt7e39bm2i0lNTdW5c+c0a9YslSt3IVxOnjxZ7dq10yuvvGKdSapQoYImT54sZ2dnhYaGqm3btkpPT7+icJWenq6tW7dqz549qlatmiRp1qxZqlevnn744Qfdeeed2r9/vwYOHKjQ0FBJUkhIiHX//fv36+GHH1ZERISkC3ezlTaHh6uEhAQdPXpUKSkpyszMVMOGDbVkyRLrBdq/f7/NNGHTpk2VmpqqoUOHavDgwQoJCdHChQtVv359SRemM7ds2aIPPvhAWVlZqlKlilq1aqXRo0cXe+sfAAAAyhhXrwszSI46dwmFhoaqadOmmj59upo3b65ff/1Vq1atst6Gl5+fr3Hjxunzzz/XwYMHlZubq5ycnBIvqLZjxw5Vq1bNGqwkFXs315w5c/Tmm29q9+7dOnXqlM6fP2/3M047duxQZGSkNVhJ0t13362CggLt2rXL+rt7vXr1bBaQCwoK0tatW+0619/PWa1aNWuwkqTw8HD5+flpx44duvPOO5WcnKwnnnhCH374oWJiYtSxY0fVqlVLktS3b1/9+9//1rJlyxQTE6OHH35YDRo0uKK2lJTDP+dKkvr06aN9+/YpJydHa9euVVRUlHXbypUrbaYaJaljx47atWuXcnJytG3bNpv7Jj09PbV06VIdOXJEubm52rt3r6ZNm1ZkhUEAAACUURbLhVvzHPEqwfNWf9ezZ099+umnOnnypGbMmKFatWqpWbNmkqTXXntNU6dO1cCBA7VixQpt3rxZsbGxpq6ImJGRoS5duqhNmzb68ssvtWnTJg0ZMqTUVl385/NJFoulVJfPHzFihH766Se1bdtWX3/9tcLDw7VgwQJJ0hNPPKHffvtNXbt21datW9W4cWNNmjSp1NoiXSfhCgAAALgRderUSU5OTkpNTdWsWbP0+OOPW5+/Wr16tdq0aaPHHntMkZGRqlmzpn7++ecSHzssLEwHDhzQoUOHrGXff/+9TZ01a9aoevXqGjJkiBo3bqyQkJAiy567ubkpPz//suf68ccfdfr0aWvZ6tWr5eTkpLp165a4zfYo7N+BAwesZdu3b1dWVpbCw8OtZXXq1NF//vMfLVu2TA899JBmzJhh3VatWjX17t1b8+fP14ABA/Tuu++WSlsLEa4AAACAUuLt7a2EhAQNGjRIhw4dUlJSknVbSEiIVqxYoTVr1mjHjh166qmniqyKfSkxMTGqU6eOunfvrh9//FGrVq3SkCFDbOqEhIRo//79mj17tnbv3q0333zTOrNTKDg4WHv27NHmzZt17Ngx5eTkFDlXly5d5OHhoe7du2vbtm1asWKFnn32WXXt2vWq7xDLz8/X5s2bbV47duxQTEyMIiIi1KVLF23cuFHr1q1Tt27d1KxZMzVu3Fhnz55Vnz59tHLlSu3bt0+rV6/WDz/8oLCwMElS//79tXTpUu3Zs0cbN27UihUrrNtKC+EKAAAAKEU9e/bUn3/+qdjYWJvno4YMGaLIyEi1bt1azZs3V2BgoHUF7ZJwcnLSggULdPbsWTVp0kRPPPGEXnzxRZs6Dz74oP7zn/+oT58+atiwodasWaNhw4bZ1Hn44YcVFxenFi1aqFKlSvr444+LnMvLy0tLly7ViRMndOedd+qRRx7R/fffr8mTJ9v3zSjGqVOndPvtt9u82rVrJ4vFos8++0wVKlTQfffdp5iYGNWsWVNz5syRdGGthePHj6tbt26qU6eOOnXqpNatW2vkyJGSLoS2Z555RmFhYYqLi1OdOnU0ZcqUq27vpVgMwzBK9QxlUHZ2tnx9ffXXX385/APNcHF5eXlatGiR2rRpw+eJoEQYM7AXYwb2YLyUnnPnzmnPnj2qUaNGmfkQ4ZIoKChQdna2dbVAOM6lxpg92YCrCAAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAIAygXXYUFrMGluEKwAAAFzXnJ2dJUm5ubkObgluVGfOnJGkq17p08WMxgAAAAClxcXFRV5eXjp69KhcXV1vmGXLCwoKlJubq3Pnzt0wfSprDMPQmTNndOTIEfn5+VmD/JUiXAEAAOC6ZrFYFBQUpD179mjfvn2Obo5pDMPQ2bNn5enpKYvF4ujm3NT8/PwUGBh41cchXAEAAOC65+bmppCQkBvq1sC8vDx9++23uu+++/jgaQdydXW96hmrQoQrAAAAlAlOTk7y8PBwdDNM4+zsrPPnz8vDw4NwdYPg5k4AAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwwXURrt566y0FBwfLw8NDUVFRWrdu3SXrz507V6GhofLw8FBERIQWLVp00bq9e/eWxWLRhAkTTG41AAAAAPwfh4erOXPmKDk5WcOHD9fGjRsVGRmp2NhYHTlypNj6a9asUWJionr27KlNmzYpPj5e8fHx2rZtW5G6CxYs0Pfff68qVaqUdjcAAAAA3OQcHq5ef/119erVSz169FB4eLimTp0qLy8vTZ8+vdj6EydOVFxcnAYOHKiwsDCNHj1ad9xxhyZPnmxT7+DBg3r22Wf10UcfydXV9Vp0BQAAAMBNzMWRJ8/NzdWGDRs0aNAga5mTk5NiYmKUkZFR7D4ZGRlKTk62KYuNjdXChQut7wsKCtS1a1cNHDhQ9erVu2w7cnJylJOTY32fnZ0tScrLy1NeXp49XcI1VHhtuEYoKcYM7MWYgT0YL7AXY6ZssOf6ODRcHTt2TPn5+QoICLApDwgI0M6dO4vdJzMzs9j6mZmZ1vevvPKKXFxc1Ldv3xK1Y+zYsRo5cmSR8mXLlsnLy6tEx4DjpKWlOboJKGMYM7AXYwb2YLzAXoyZ69uZM2dKXNeh4ao0bNiwQRMnTtTGjRtlsVhKtM+gQYNsZsOys7NVrVo1tWrVSj4+PqXVVFylvLw8paWlqWXLltz6iRJhzMBejBnYg/ECezFmyobCu9pKwqHhqmLFinJ2dtbhw4dtyg8fPqzAwMBi9wkMDLxk/VWrVunIkSO67bbbrNvz8/M1YMAATZgwQXv37i1yTHd3d7m7uxcpd3V1ZaCXAVwn2IsxA3sxZmAPxgvsxZi5vtlzbRy6oIWbm5saNWqk9PR0a1lBQYHS09MVHR1d7D7R0dE29aULU6mF9bt27aotW7Zo8+bN1leVKlU0cOBALV26tPQ6AwAAAOCm5vDbApOTk9W9e3c1btxYTZo00YQJE3T69Gn16NFDktStWzfdeuutGjt2rCSpX79+atasmcaPH6+2bdtq9uzZWr9+vaZNmyZJ8vf3l7+/v805XF1dFRgYqLp1617bzgEAAAC4aTg8XCUkJOjo0aNKSUlRZmamGjZsqCVLllgXrdi/f7+cnP5vgq1p06ZKTU3V0KFDNXjwYIWEhGjhwoWqX7++o7oAAAAAAI4PV5LUp08f9enTp9htK1euLFLWsWNHdezYscTHL+45KwAAAAAwk8M/RBgAAAAAbgSEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATHBdhKu33npLwcHB8vDwUFRUlNatW3fJ+nPnzlVoaKg8PDwUERGhRYsW2WwfMWKEQkNDVa5cOVWoUEExMTFau3ZtaXYBAAAAwE3O4eFqzpw5Sk5O1vDhw7Vx40ZFRkYqNjZWR44cKbb+mjVrlJiYqJ49e2rTpk2Kj49XfHy8tm3bZq1Tp04dTZ48WVu3btV3332n4OBgtWrVSkePHr1W3QIAAABwk3F4uHr99dfVq1cv9ejRQ+Hh4Zo6daq8vLw0ffr0YutPnDhRcXFxGjhwoMLCwjR69Gjdcccdmjx5srXOo48+qpiYGNWsWVP16tXT66+/ruzsbG3ZsuVadQsAAADATcbFkSfPzc3Vhg0bNGjQIGuZk5OTYmJilJGRUew+GRkZSk5OtimLjY3VwoULL3qOadOmydfXV5GRkcXWycnJUU5OjvV9dna2JCkvL095eXn2dAnXUOG14RqhpBgzsBdjBvZgvMBejJmywZ7r49BwdezYMeXn5ysgIMCmPCAgQDt37ix2n8zMzGLrZ2Zm2pR9+eWX6ty5s86cOaOgoCClpaWpYsWKxR5z7NixGjlyZJHyZcuWycvLy54uwQHS0tIc3QSUMYwZ2IsxA3swXmAvxsz17cyZMyWu69BwVZpatGihzZs369ixY3r33XfVqVMnrV27VpUrVy5Sd9CgQTazYdnZ2apWrZpatWolHx+fa9ls2CEvL09paWlq2bKlXF1dHd0clAGMGdiLMQN7MF5gL8ZM2VB4V1tJODRcVaxYUc7Ozjp8+LBN+eHDhxUYGFjsPoGBgSWqX65cOdWuXVu1a9fWXXfdpZCQEL3//vs2tyAWcnd3l7u7e5FyV1dXBnoZwHWCvRgzsBdjBvZgvMBejJnrmz3XxqELWri5ualRo0ZKT0+3lhUUFCg9PV3R0dHF7hMdHW1TX7owlXqx+n8/7t+fqwIAAAAAMzn8tsDk5GR1795djRs3VpMmTTRhwgSdPn1aPXr0kCR169ZNt956q8aOHStJ6tevn5o1a6bx48erbdu2mj17ttavX69p06ZJkk6fPq0XX3xRDz74oIKCgnTs2DG99dZbOnjwoDp27OiwfgIAAAC4sTk8XCUkJOjo0aNKSUlRZmamGjZsqCVLllgXrdi/f7+cnP5vgq1p06ZKTU3V0KFDNXjwYIWEhGjhwoWqX7++JMnZ2Vk7d+7UBx98oGPHjsnf31933nmnVq1apXr16jmkjwAAAABufFcUrg4cOCCLxaKqVatKktatW6fU1FSFh4frySeftPt4ffr0UZ8+fYrdtnLlyiJlHTt2vOgslIeHh+bPn293GwAAAADgalzRM1ePPvqoVqxYIenC0ugtW7bUunXrNGTIEI0aNcrUBgIAAABAWXBF4Wrbtm1q0qSJJOmTTz5R/fr1tWbNGn300UeaOXOmme0DAAAAgDLhisJVXl6edeny5cuX68EHH5QkhYaG6tChQ+a1DgAAAADKiCsKV/Xq1dPUqVO1atUqpaWlKS4uTpL0xx9/yN/f39QGAgAAAEBZcEXh6pVXXtE777yj5s2bKzExUZGRkZKkzz//3Hq7IAAAAADcTK5otcDmzZvr2LFjys7OVoUKFazlTz75pLy8vExrHAAAAACUFVc0c3X27Fnl5ORYg9W+ffs0YcIE7dq1S5UrVza1gQAAAABQFlxRuGrfvr1mzZolScrKylJUVJTGjx+v+Ph4vf3226Y2EAAAAADKgisKVxs3btS9994rSZo3b54CAgK0b98+zZo1S2+++aapDQQAAACAsuCKwtWZM2dUvnx5SdKyZcv00EMPycnJSXfddZf27dtnagMBAAAAoCy4onBVu3ZtLVy4UAcOHNDSpUvVqlUrSdKRI0fk4+NjagMBAAAAoCy4onCVkpKi5557TsHBwWrSpImio6MlXZjFuv32201tIAAAAACUBVe0FPsjjzyie+65R4cOHbJ+xpUk3X///erQoYNpjQMAAACAsuKKwpUkBQYGKjAwUL///rskqWrVqnyAMAAAAICb1hXdFlhQUKBRo0bJ19dX1atXV/Xq1eXn56fRo0eroKDA7DYCAAAAwHXvimauhgwZovfff18vv/yy7r77bknSd999pxEjRujcuXN68cUXTW0kAAAAAFzvrihcffDBB3rvvff04IMPWssaNGigW2+9VU8//TThCgAAAMBN54puCzxx4oRCQ0OLlIeGhurEiRNX3SgAAAAAKGuuKFxFRkZq8uTJRconT56sBg0aXHWjAAAAAKCsuaLbAl999VW1bdtWy5cvt37GVUZGhg4cOKBFixaZ2kAAAAAAKAuuaOaqWbNm+vnnn9WhQwdlZWUpKytLDz30kH766Sd9+OGHZrcRAAAAAK57V/w5V1WqVCmycMWPP/6o999/X9OmTbvqhgEAAABAWXJFM1cAAAAAAFuEKwAAAAAwAeEKAAAAAExg1zNXDz300CW3Z2VlXU1bAAAAAKDMsitc+fr6XnZ7t27drqpBAAAAAFAW2RWuZsyYUVrtAAAAAIAyjWeuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATHBdhKu33npLwcHB8vDwUFRUlNatW3fJ+nPnzlVoaKg8PDwUERGhRYsWWbfl5eXp+eefV0REhMqVK6cqVaqoW7du+uOPP0q7GwAAAABuYg4PV3PmzFFycrKGDx+ujRs3KjIyUrGxsTpy5Eix9desWaPExET17NlTmzZtUnx8vOLj47Vt2zZJ0pkzZ7Rx40YNGzZMGzdu1Pz587Vr1y49+OCD17JbAAAAAG4yDg9Xr7/+unr16qUePXooPDxcU6dOlZeXl6ZPn15s/YkTJyouLk4DBw5UWFiYRo8erTvuuEOTJ0+WJPn6+iotLU2dOnVS3bp1ddddd2ny5MnasGGD9u/ffy27BgAAAOAm4uLIk+fm5mrDhg0aNGiQtczJyUkxMTHKyMgodp+MjAwlJyfblMXGxmrhwoUXPc9ff/0li8UiPz+/Yrfn5OQoJyfH+j47O1vShVsM8/LyStgbXGuF14ZrhJJizMBejBnYg/ECezFmygZ7ro9Dw9WxY8eUn5+vgIAAm/KAgADt3Lmz2H0yMzOLrZ+ZmVls/XPnzun5559XYmKifHx8iq0zduxYjRw5skj5smXL5OXlVZKuwIHS0tIc3QSUMYwZ2IsxA3swXmAvxsz17cyZMyWu69BwVdry8vLUqVMnGYaht99++6L1Bg0aZDMblp2drWrVqqlVq1YXDWRwvLy8PKWlpally5ZydXV1dHNQBjBmYC/GDOzBeIG9GDNlQ+FdbSXh0HBVsWJFOTs76/Dhwzblhw8fVmBgYLH7BAYGlqh+YbDat2+fvv7660uGJHd3d7m7uxcpd3V1ZaCXAVwn2IsxA3sxZmAPxgvsxZi5vtlzbRy6oIWbm5saNWqk9PR0a1lBQYHS09MVHR1d7D7R0dE29aULU6l/r18YrH755RctX75c/v7+pdMBAAAAAPj/HH5bYHJysrp3767GjRurSZMmmjBhgk6fPq0ePXpIkrp166Zbb71VY8eOlST169dPzZo10/jx49W2bVvNnj1b69ev17Rp0yRdCFaPPPKINm7cqC+//FL5+fnW57FuueUWubm5OaajAAAAAG5oDg9XCQkJOnr0qFJSUpSZmamGDRtqyZIl1kUr9u/fLyen/5tga9q0qVJTUzV06FANHjxYISEhWrhwoerXry9JOnjwoD7//HNJUsOGDW3OtWLFCjVv3vya9AsAAADAzcXh4UqS+vTpoz59+hS7beXKlUXKOnbsqI4dOxZbPzg4WIZhmNk8AAAAALgsh3+IMAAAAADcCAhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYwOHh6q233lJwcLA8PDwUFRWldevWXbL+3LlzFRoaKg8PD0VERGjRokU22+fPn69WrVrJ399fFotFmzdvLsXWAwAAAMAFDg1Xc+bMUXJysoYPH66NGzcqMjJSsbGxOnLkSLH116xZo8TERPXs2VObNm1SfHy84uPjtW3bNmud06dP65577tErr7xyrboBAAAAAI4NV6+//rp69eqlHj16KDw8XFOnTpWXl5emT59ebP2JEycqLi5OAwcOVFhYmEaPHq077rhDkydPttbp2rWrUlJSFBMTc626AQAAAABycdSJc3NztWHDBg0aNMha5uTkpJiYGGVkZBS7T0ZGhpKTk23KYmNjtXDhwqtqS05OjnJycqzvs7OzJUl5eXnKy8u7qmOj9BReG64RSooxA3sxZmAPxgvsxZgpG+y5Pg4LV8eOHVN+fr4CAgJsygMCArRz585i98nMzCy2fmZm5lW1ZezYsRo5cmSR8mXLlsnLy+uqjo3Sl5aW5ugmoIxhzMBejBnYg/ECezFmrm9nzpwpcV2HhavryaBBg2xmxLKzs1WtWjW1atVKPj4+DmwZLiUvL09paWlq2bKlXF1dHd0clAGMGdiLMQN7MF5gL8ZM2VB4V1tJOCxcVaxYUc7Ozjp8+LBN+eHDhxUYGFjsPoGBgXbVLyl3d3e5u7sXKXd1dWWglwFcJ9iLMQN7MWZgD8YL7MWYub7Zc20ctqCFm5ubGjVqpPT0dGtZQUGB0tPTFR0dXew+0dHRNvWlC9OoF6sPAAAAANeKQ28LTE5OVvfu3dW4cWM1adJEEyZM0OnTp9WjRw9JUrdu3XTrrbdq7NixkqR+/fqpWbNmGj9+vNq2bavZs2dr/fr1mjZtmvWYJ06c0P79+/XHH39Iknbt2iXpwqzX1c5wAQAAAMDFODRcJSQk6OjRo0pJSVFmZqYaNmyoJUuWWBet2L9/v5yc/m9yrWnTpkpNTdXQoUM1ePBghYSEaOHChapfv761zueff24NZ5LUuXNnSdLw4cM1YsSIa9MxAAAAADcdhy9o0adPH/Xp06fYbStXrixS1rFjR3Xs2PGix0tKSlJSUpJJrQMAAACAknHohwgDAAAAwI2CcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAmui3D11ltvKTg4WB4eHoqKitK6desuWX/u3LkKDQ2Vh4eHIiIitGjRIpvthmEoJSVFQUFB8vT0VExMjH755ZfS7AIAAACAm5zDw9WcOXOUnJys4cOHa+PGjYqMjFRsbKyOHDlSbP01a9YoMTFRPXv21KZNmxQfH6/4+Hht27bNWufVV1/Vm2++qalTp2rt2rUqV66cYmNjde7cuWvVLQAAAAA3GYeHq9dff129evVSjx49FB4erqlTp8rLy0vTp08vtv7EiRMVFxengQMHKiwsTKNHj9Ydd9yhyZMnS7owazVhwgQNHTpU7du3V4MGDTRr1iz98ccfWrhw4TXsGQAAAICbiYsjT56bm6sNGzZo0KBB1jInJyfFxMQoIyOj2H0yMjKUnJxsUxYbG2sNTnv27FFmZqZiYmKs2319fRUVFaWMjAx17ty5yDFzcnKUk5Njff/XX39Jkk6cOKG8vLwr7h9KV15ens6cOaPjx4/L1dXV0c1BGcCYgb0YM7AH4wX2YsyUDSdPnpR0YRLnchwaro4dO6b8/HwFBATYlAcEBGjnzp3F7pOZmVls/czMTOv2wrKL1fmnsWPHauTIkUXKa9SoUbKOAAAAALihnTx5Ur6+vpes49Bwdb0YNGiQzWxYQUGBTpw4IX9/f1ksFge2DJeSnZ2tatWq6cCBA/Lx8XF0c1AGMGZgL8YM7MF4gb0YM2WDYRg6efKkqlSpctm6Dg1XFStWlLOzsw4fPmxTfvjwYQUGBha7T2Bg4CXrF/738OHDCgoKsqnTsGHDYo/p7u4ud3d3mzI/Pz97ugIH8vHx4QcS7MKYgb0YM7AH4wX2Ysxc/y43Y1XIoQtauLm5qVGjRkpPT7eWFRQUKD09XdHR0cXuEx0dbVNfktLS0qz1a9SoocDAQJs62dnZWrt27UWPCQAAAABXy+G3BSYnJ6t79+5q3LixmjRpogkTJuj06dPq0aOHJKlbt2669dZbNXbsWElSv3791KxZM40fP15t27bV7NmztX79ek2bNk2SZLFY1L9/f40ZM0YhISGqUaOGhg0bpipVqig+Pt5R3QQAAABwg3N4uEpISNDRo0eVkpKizMxMNWzYUEuWLLEuSLF//345Of3fBFvTpk2VmpqqoUOHavDgwQoJCdHChQtVv359a53//ve/On36tJ588kllZWXpnnvu0ZIlS+Th4XHN+4fS4+7uruHDhxe5pRO4GMYM7MWYgT0YL7AXY+bGYzFKsqYgAAAAAOCSHP4hwgAAAABwIyBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHCF69aJEyfUpUsX+fj4yM/PTz179tSpU6cuuc+5c+f0zDPPyN/fX97e3nr44YeLfOh0oePHj6tq1aqyWCzKysoqhR7gWiuNMfPjjz8qMTFR1apVk6enp8LCwjRx4sTS7gpKyVtvvaXg4GB5eHgoKipK69atu2T9uXPnKjQ0VB4eHoqIiNCiRYtsthuGoZSUFAUFBcnT01MxMTH65ZdfSrMLuMbMHDN5eXl6/vnnFRERoXLlyqlKlSrq1q2b/vjjj9LuBq4hs3/O/F3v3r1lsVg0YcIEk1sN0xjAdSouLs6IjIw0vv/+e2PVqlVG7dq1jcTExEvu07t3b6NatWpGenq6sX79euOuu+4ymjZtWmzd9u3bG61btzYkGX/++Wcp9ADXWmmMmffff9/o27evsXLlSmP37t3Ghx9+aHh6ehqTJk0q7e7AZLNnzzbc3NyM6dOnGz/99JPRq1cvw8/Pzzh8+HCx9VevXm04Ozsbr776qrF9+3Zj6NChhqurq7F161ZrnZdfftnw9fU1Fi5caPz444/Ggw8+aNSoUcM4e/bsteoWSpHZYyYrK8uIiYkx5syZY+zcudPIyMgwmjRpYjRq1OhadgulqDR+zhSaP3++ERkZaVSpUsV44403SrknuFKEK1yXtm/fbkgyfvjhB2vZ4sWLDYvFYhw8eLDYfbKysgxXV1dj7ty51rIdO3YYkoyMjAybulOmTDGaNWtmpKenE65uEKU9Zv7u6aefNlq0aGFe43FNNGnSxHjmmWes7/Pz840qVaoYY8eOLbZ+p06djLZt29qURUVFGU899ZRhGIZRUFBgBAYGGuPGjbNuz8rKMtzd3Y2PP/64FHqAa83sMVOcdevWGZKMffv2mdNoOFRpjZnff//duPXWW41t27YZ1atXJ1xdx7gtENeljIwM+fn5qXHjxtaymJgYOTk5ae3atcXus2HDBuXl5SkmJsZaFhoaqttuu00ZGRnWsu3bt2vUqFGaNWuWzQdUo2wrzTHzT3/99ZduueUW8xqPUpebm6sNGzbYXGsnJyfFxMRc9FpnZGTY1Jek2NhYa/09e/YoMzPTpo6vr6+ioqIuOX5QNpTGmCnOX3/9JYvFIj8/P1PaDccprTFTUFCgrl27auDAgapXr17pNB6m4TdLXJcyMzNVuXJlmzIXFxfdcsstyszMvOg+bm5uRf4HFRAQYN0nJydHiYmJGjdunG677bZSaTsco7TGzD+tWbNGc+bM0ZNPPmlKu3FtHDt2TPn5+QoICLApv9S1zszMvGT9wv/ac0yUHaUxZv7p3Llzev7555WYmCgfHx9zGg6HKa0x88orr8jFxUV9+/Y1v9EwHeEK19QLL7wgi8VyydfOnTtL7fyDBg1SWFiYHnvssVI7B8zl6DHzd9u2bVP79u01fPhwtWrV6pqcE8CNKS8vT506dZJhGHr77bcd3RxcpzZs2KCJEydq5syZslgsjm4OSsDF0Q3AzWXAgAFKSkq6ZJ2aNWsqMDBQR44csSk/f/68Tpw4ocDAwGL3CwwMVG5urrKysmxmIg4fPmzd5+uvv9bWrVs1b948SRdW+pKkihUrasiQIRo5cuQV9gylxdFjptD27dt1//3368knn9TQoUOvqC9wnIoVK8rZ2bnI6qHFXetCgYGBl6xf+N/Dhw8rKCjIpk7Dhg1NbD0coTTGTKHCYLVv3z59/fXXzFrdIEpjzKxatUpHjhyxudsmPz9fAwYM0IQJE7R3715zO4GrxswVrqlKlSopNDT0ki83NzdFR0crKytLGzZssO779ddfq6CgQFFRUcUeu1GjRnJ1dVV6erq1bNeuXdq/f7+io6MlSZ9++ql+/PFHbd68WZs3b9Z7770n6cIPr2eeeaYUe44r5egxI0k//fSTWrRooe7du+vFF18svc6i1Li5ualRo0Y217qgoEDp6ek21/rvoqOjbepLUlpamrV+jRo1FBgYaFMnOztba9euvegxUXaUxpiR/i9Y/fLLL1q+fLn8/f1LpwO45kpjzHTt2lVbtmyx/t6yefNmValSRQMHDtTSpUtLrzO4co5eUQO4mLi4OOP222831q5da3z33XdGSEiIzbLav//+u1G3bl1j7dq11rLevXsbt912m/H1118b69evN6Kjo43o6OiLnmPFihWsFngDKY0xs3XrVqNSpUrGY489Zhw6dMj6OnLkyDXtG67e7NmzDXd3d2PmzJnG9u3bjSeffNLw8/MzMjMzDcMwjK5duxovvPCCtf7q1asNFxcX47XXXjN27NhhDB8+vNil2P38/IzPPvvM2LJli9G+fXuWYr+BmD1mcnNzjQcffNCoWrWqsXnzZpufKTk5OQ7pI8xVGj9n/onVAq9vhCtct44fP24kJiYa3t7eho+Pj9GjRw/j5MmT1u179uwxJBkrVqywlp09e9Z4+umnjQoVKhheXl5Ghw4djEOHDl30HISrG0tpjJnhw4cbkoq8qlevfg17BrNMmjTJuO222ww3NzejSZMmxvfff2/d1qxZM6N79+429T/55BOjTp06hpubm1GvXj3jq6++stleUFBgDBs2zAgICDDc3d2N+++/39i1a9e16AquETPHTOHPoOJef/+5hLLN7J8z/0S4ur5ZDOP/P3QCAAAAALhiPHMFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAKDP27t0ri8WizZs3l/q5Zs6cKT8/v1I/DwDgxkG4AgCYIikpSRaLpcgrLi7O0U27rODgYE2YMMGmLCEhQT///HOpn3vPnj169NFHVaVKFXl4eKhq1apq3769du7cKenaBkoAwNVxcXQDAAA3jri4OM2YMcOmzN3d3UGtuTqenp7y9PQs1XPk5eWpZcuWqlu3rubPn6+goCD9/vvvWrx4sbKyskr13AAA8zFzBQAwjbu7uwIDA21eFSpUkCQ9+uijSkhIsKmfl5enihUratasWZKkJUuW6J577pGfn5/8/f31wAMPaPfu3Rc9X3G37i1cuFAWi8X6fvfu3Wrfvr0CAgLk7e2tO++8U8uXL7dub968ufbt26f//Oc/1tm2ix377bffVq1ateTm5qa6devqww8/tNlusVj03nvvqUOHDvLy8lJISIg+//zzi7b/p59+0u7duzVlyhTdddddql69uu6++26NGTNGd911lySpRo0akqTbb79dFotFzZs3t+7/3nvvKSwsTB4eHgoNDdWUKVOs2wpnvGbPnq2mTZvKw8ND9evX1zfffHPR9gAArg7hCgBwTXTp0kVffPGFTp06ZS1bunSpzpw5ow4dOkiSTp8+reTkZK1fv17p6elycnJShw4dVFBQcMXnPXXqlNq0aaP09HRt2rRJcXFxateunfbv3y9Jmj9/vqpWrapRo0bp0KFDOnToULHHWbBggfr166cBAwZo27Zteuqpp9SjRw+tWLHCpt7IkSPVqVMnbdmyRW3atFGXLl104sSJYo9ZqVIlOTk5ad68ecrPzy+2zrp16yRJy5cv16FDhzR//nxJ0kcffaSUlBS9+OKL2rFjh1566SUNGzZMH3zwgc3+AwcO1IABA7Rp0yZFR0erXbt2On78eMm/gQCAkjMAADBB9+7dDWdnZ6NcuXI2rxdffNEwDMPIy8szKlasaMyaNcu6T2JiopGQkHDRYx49etSQZGzdutUwDMPYs2ePIcnYtGmTYRiGMWPGDMPX19dmnwULFhiX+99bvXr1jEmTJlnfV69e3XjjjTds6vzz2E2bNjV69eplU6djx45GmzZtrO8lGUOHDrW+P3XqlCHJWLx48UXbMnnyZMPLy8soX7680aJFC2PUqFHG7t27rdv/2edCtWrVMlJTU23KRo8ebURHR9vs9/LLL1u35+XlGVWrVjVeeeWVi7YHAHDlmLkCAJimRYsW2rx5s82rd+/ekiQXFxd16tRJH330kaQLs1SfffaZunTpYt3/l19+UWJiomrWrCkfHx8FBwdLknWW6UqcOnVKzz33nMLCwuTn5ydvb2/t2LHD7mPu2LFDd999t03Z3XffrR07dtiUNWjQwPp1uXLl5OPjoyNHjlz0uM8884wyMzP10UcfKTo6WnPnzlW9evWUlpZ20X1Onz6t3bt3q2fPnvL29ra+xowZU+Q2yujoaOvXLi4uaty4cZE2AwDMwYIWAADTlCtXTrVr177o9i5duqhZs2Y6cuSI0tLS5OnpabOaYLt27VS9enW9++67qlKligoKClS/fn3l5uYWezwnJycZhmFTlpeXZ/P+ueeeU1paml577TXVrl1bnp6eeuSRRy56zKvl6upq895isVz2tsby5curXbt2ateuncaMGaPY2FiNGTNGLVu2LLZ+4a2V7777rqKiomy2OTs7X0XrAQBXg5krAMA107RpU1WrVk1z5szRRx99pI4dO1rDyPHjx7Vr1y4NHTpU999/v8LCwvTnn39e8niVKlXSyZMndfr0aWvZP5csX716tZKSktShQwdFREQoMDBQe/futanj5uZ20WeeCoWFhWn16tVFjh0eHn6ZXtvHYrEoNDTU2ic3NzdJsmlfQECAqlSpot9++021a9e2eRUugFHo+++/t359/vx5bdiwQWFhYaa2GQBwATNXAADT5OTkKDMz06bMxcVFFStWtL5/9NFHNXXqVP388882i0FUqFBB/v7+mjZtmoKCgrR//3698MILlzxfVFSUvLy8NHjwYPXt21dr167VzJkzbeqEhIRo/vz5ateunSwWi4YNG1ZkJik4OFjffvutOnfuLHd3d5v2Fho4cKA6deqk22+/XTExMfriiy80f/58m5UH7bV582YNHz5cXbt2VXh4uNzc3PTNN99o+vTpev755yVJlStXlqenp5YsWaKqVavKw8NDvr6+GjlypPr27StfX1/FxcUpJydH69ev159//qnk5GTrOd566y2FhIQoLCxMb7zxhv788089/vjjV9xmAMAlOPqhLwDAjaF79+6GpCKvunXr2tTbvn27IcmoXr26UVBQYLMtLS3NCAsLM9zd3Y0GDRoYK1euNCQZCxYsMAyj+MUdFixYYNSuXdvw9PQ0HnjgAWPatGk2C1rs2bPHaNGiheHp6WlUq1bNmDx5stGsWTOjX79+1joZGRlGgwYNDHd3d+u+xS2WMWXKFKNmzZqGq6urUadOHZvFOQzDsGlrIV9fX2PGjBnFfs+OHj1q9O3b16hfv77h7e1tlC9f3oiIiDBee+01Iz8/31rv3XffNapVq2Y4OTkZzZo1s5Z/9NFHRsOGDQ03NzejQoUKxn333WfMnz/f5nuVmppqNGnSxHBzczPCw8ONr7/+uti2AACunsUw/nGzOgAAKPP27t2rGjVqaNOmTWrYsKGjmwMANwWeuQIAAAAAExCuAAAAAMAE3BYIAAAAACZg5goAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMMH/A5+SVNvbYsq1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylim(0)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Plotting Training and Validation Loss\n",
    "\n",
    "This code visualizes how the training and validation losses change over time during model training.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt  # Import the plotting library\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 5))     # Create a figure of size 10x5 inches\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.plot(train_losses, label=\"Train Loss\")        # Plot training loss curve\n",
    "plt.plot(val_losses, label=\"Validation Loss\")     # Plot validation loss curve\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.xlabel(\"Evaluation Step\")      # Label for the x-axis\n",
    "plt.ylim(0)                       # Set the y-axis to start from 0 (no negative loss)\n",
    "plt.ylabel(\"Loss\")                # Label for the y-axis\n",
    "plt.title(\"Training and Validation Loss Over Time\")  # Chart title\n",
    "plt.legend()                     # Show legend to distinguish train/val curves\n",
    "plt.grid()                       # Add grid lines for better readability\n",
    "plt.show()                       # Display the plot\n",
    "```\n",
    "\n",
    "### What You’ll See\n",
    "\n",
    "- A line graph with two curves:\n",
    "  - **Train Loss** decreasing as the model learns.\n",
    "  - **Validation Loss** showing how well the model performs on unseen data.\n",
    "- The x-axis shows evaluation steps (every time loss was calculated).\n",
    "- The y-axis shows the loss values.\n",
    "\n",
    "This helps you visually check if the model is improving and if it’s overfitting or underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(\"Salam labas\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Generating Text with the Model\n",
    "\n",
    "This code takes an input text, processes it, and uses the model to generate new text based on that input.\n",
    "\n",
    "```python\n",
    "input_tokens = tokenizer.encode(\"Salam labas\")\n",
    "```\n",
    "- Converts the input text `\"Salam labas\"` into a list of token IDs using the tokenizer.\n",
    "\n",
    "```python\n",
    "input_tokens = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "```\n",
    "- Converts the list of tokens into a PyTorch tensor.\n",
    "- `.unsqueeze(0)` adds a batch dimension, making it shape `(1, sequence_length)`.\n",
    "- Moves the tensor to the device (CPU or GPU) where the model is located.\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "- Sets the model to evaluation mode (disables dropout and other training behaviors).\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=50)\n",
    "```\n",
    "- Disables gradient computation (since we’re only generating text, not training).\n",
    "- Uses the model’s `.generate()` method to produce up to 50 new tokens based on the input.\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(output[0].tolist()))\n",
    "```\n",
    "- Converts the generated token IDs back to text.\n",
    "- Prints the generated text output.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "This snippet generates text starting from `\"Salam labas\"` and continues for 50 tokens, showing the model’s prediction in natural language.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
