{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/my_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: BasicTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size = 512\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/combined_text.txt\", \"r\") as f:\n",
    "    text_sequence = f.read()\n",
    "\n",
    "encoded_text_sequence = tokenizer.encode(text_sequence)\n",
    "len(encoded_text_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoded_text_sequence, dtype=torch.long)\n",
    "split_index = int(0.9*len(data))\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìÑ Function: `get_batch(split: str)`\n",
    "\n",
    "This function creates a small batch of input and target sequences for training or validation. It returns two tensors: `x` (inputs) and `y` (targets), both ready to be fed into a model.\n",
    "\n",
    "### üîß Code Breakdown\n",
    "\n",
    "```python\n",
    "from typing import Tuple\n",
    "```\n",
    "\n",
    "* This line allows us to specify that the function will return a **tuple** of two PyTorch tensors.\n",
    "\n",
    "```python\n",
    "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "```\n",
    "\n",
    "* Defines the function `get_batch` that takes a string `split`, which should be either `'train'` or `'val'`.\n",
    "* It returns two PyTorch tensors: the input batch `x` and the target batch `y`.\n",
    "\n",
    "```python\n",
    "    data = train_data if split == 'train' else val_data\n",
    "```\n",
    "\n",
    "* Depending on the `split`, it selects the training or validation data.\n",
    "\n",
    "```python\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "```\n",
    "\n",
    "* Randomly selects `batch_size` starting positions from the data to generate sequences.\n",
    "* `block_size` is the length of each input sequence.\n",
    "\n",
    "```python\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "```\n",
    "\n",
    "* For each index, extract a sequence of length `block_size` from the data.\n",
    "* These sequences are stacked into a batch `x`.\n",
    "\n",
    "```python\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
    "```\n",
    "\n",
    "* Creates the target batch `y`, which is the same as `x` but shifted one position forward. This is typically used in language models to predict the next token.\n",
    "\n",
    "```python\n",
    "    x, y = x.to(device), y.to(device)\n",
    "```\n",
    "\n",
    "* Moves the tensors to the desired computing device (CPU or GPU).\n",
    "\n",
    "```python\n",
    "    return x, y\n",
    "```\n",
    "\n",
    "* Returns the input and target batches.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Summary\n",
    "\n",
    "* The function creates batches of sequences for model training or evaluation.\n",
    "* Each input `x` is a chunk of tokens, and the corresponding `y` is the same chunk shifted one token ahead.\n",
    "* It's commonly used for training autoregressive models like GPT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch('train')\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üß™ Using the `get_batch` Function\n",
    "\n",
    "```python\n",
    "x, y = get_batch('train')\n",
    "x.shape, y.shape\n",
    "```\n",
    "\n",
    "### üîç What This Does:\n",
    "\n",
    "* Calls the `get_batch` function with `'train'` to get a **training batch**.\n",
    "* `x` is the **input tensor**.\n",
    "* `y` is the **target tensor**, which is the input shifted one position ahead.\n",
    "* `.shape` returns the shape (dimensions) of the tensors.\n",
    "\n",
    "### üìê Expected Output Shape:\n",
    "\n",
    "Assuming the following variables are defined:\n",
    "\n",
    "```python\n",
    "batch_size = 32        # number of sequences in one batch\n",
    "block_size = 128       # length of each sequence\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "x.shape == (32, 128)\n",
    "y.shape == (32, 128)\n",
    "```\n",
    "\n",
    "Each batch contains:\n",
    "\n",
    "* `32` sequences\n",
    "* Each of length `128` tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "eval_iters = 200\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss() -> Dict:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        output[split] = losses.mean()\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üìÑ Function: `estimate_loss()`\n",
    "\n",
    "This function estimates the **average loss** of the model on both the **training** and **validation** datasets. It's typically used to monitor how well the model is learning.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Code Explanation\n",
    "\n",
    "```python\n",
    "from typing import Dict\n",
    "```\n",
    "\n",
    "* This allows the function to specify that it returns a dictionary (i.e. `Dict`).\n",
    "\n",
    "```python\n",
    "eval_iters = 200\n",
    "```\n",
    "\n",
    "* The number of iterations (batches) used to estimate the average loss for each split (`train` and `val`).\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "```\n",
    "\n",
    "* A decorator that tells PyTorch **not to compute gradients**. This saves memory and speeds up the evaluation process, since we're not training here‚Äîjust measuring performance.\n",
    "\n",
    "```python\n",
    "def estimate_loss() -> Dict:\n",
    "```\n",
    "\n",
    "* Defines the function `estimate_loss`, which returns a dictionary with average losses for `'train'` and `'val'`.\n",
    "\n",
    "```python\n",
    "    output = {}\n",
    "```\n",
    "\n",
    "* An empty dictionary to store the average losses.\n",
    "\n",
    "```python\n",
    "    model.eval()\n",
    "```\n",
    "\n",
    "* Sets the model to **evaluation mode**, which turns off dropout and other training-specific behaviors.\n",
    "\n",
    "```python\n",
    "    for split in ['train', 'val']:\n",
    "```\n",
    "\n",
    "* Loops over the two data splits: `'train'` and `'val'`.\n",
    "\n",
    "```python\n",
    "        losses = torch.zeros(eval_iters)\n",
    "```\n",
    "\n",
    "* Creates a tensor to store the loss values for each of the `eval_iters` iterations.\n",
    "\n",
    "```python\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "```\n",
    "\n",
    "* For each iteration:\n",
    "\n",
    "  * Gets a batch of input (`x`) and target (`y`) data.\n",
    "  * Feeds them into the model.\n",
    "  * Extracts the loss and stores it in the `losses` tensor.\n",
    "\n",
    "```python\n",
    "        output[split] = losses.mean()\n",
    "```\n",
    "\n",
    "* Calculates the average loss for the split and stores it in the `output` dictionary.\n",
    "\n",
    "```python\n",
    "    model.train()\n",
    "```\n",
    "\n",
    "* Puts the model back into **training mode**, so it's ready to continue training after the evaluation.\n",
    "\n",
    "```python\n",
    "    return output\n",
    "```\n",
    "\n",
    "* Returns the dictionary with average training and validation losses.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Example Output\n",
    "\n",
    "After calling `estimate_loss()`, you might get something like:\n",
    "\n",
    "```python\n",
    "{'train': 1.923, 'val': 2.140}\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "* The model's average training loss is `1.923`.\n",
    "* The model's average validation loss is `2.140`.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What It Does\n",
    "\n",
    "- Disables gradient tracking with `@torch.no_grad()` for efficient evaluation.\n",
    "- Loops through `eval_iters` times to compute average loss on both `train` and `val` datasets.\n",
    "- Switches the model to **evaluation mode** during loss estimation and back to **training mode** afterward.\n",
    "- Returns a dictionary like: `{'train': 1.923, 'val': 2.140}`\n",
    "\n",
    "### üìà Purpose\n",
    "\n",
    "Useful for monitoring how well the model is learning, without affecting its gradients or training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üíæ Function: `save_checkpoint(...)`\n",
    "\n",
    "This function **saves the model's state** and other important training info (like the optimizer and loss) into a file. It's used to **resume training later** or **keep a backup** of a trained model.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Code Explanation\n",
    "\n",
    "```python\n",
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "```\n",
    "\n",
    "* Defines the `save_checkpoint` function.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `model`: The GPT model to save.\n",
    "  * `optimizer`: The optimizer used during training (e.g. Adam).\n",
    "  * `epoch`: The current epoch number (for resuming training).\n",
    "  * `loss`: The latest loss value (for reference).\n",
    "  * `file_path`: The file name to save the checkpoint (default: `checkpoint.pth`).\n",
    "\n",
    "```python\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "```\n",
    "\n",
    "* Creates a dictionary called `checkpoint` containing:\n",
    "\n",
    "  * The current epoch number\n",
    "  * The model's state (weights and parameters)\n",
    "  * The optimizer's state (learning rates, momentum, etc.)\n",
    "  * The current loss value\n",
    "\n",
    "```python\n",
    "    torch.save(checkpoint, file_path)\n",
    "```\n",
    "\n",
    "* Uses PyTorch‚Äôs `torch.save` function to write the `checkpoint` to disk.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ What This Is Used For\n",
    "\n",
    "Saving a checkpoint allows you to:\n",
    "\n",
    "* Resume training from where you left off.\n",
    "* Avoid losing progress due to crashes or interruptions.\n",
    "* Load the model later for evaluation or inference.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-4\n",
    "save_interval = 100\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iteration}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iteration % save_interval == 0:\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=iteration,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_1/checkpoint_{iteration}.pth\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Training Loop (Line-by-Line Explanation)\n",
    "\n",
    "This code trains a language model using the AdamW optimizer. It evaluates the loss at regular intervals, saves the model periodically, and collects training/validation losses for monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Setup\n",
    "\n",
    "```python\n",
    "max_iters = 1000                # Total number of training iterations (batches)\n",
    "eval_interval = 10              # How often (in iterations) to evaluate and print loss\n",
    "learning_rate = 1e-4            # Learning rate for the optimizer\n",
    "save_interval = 100             # How often (in iterations) to save the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # Initialize the optimizer\n",
    "```\n",
    "\n",
    "- We train the model for 1000 steps.\n",
    "- Every 10 steps, we evaluate how well the model is doing.\n",
    "- Every 100 steps, we save the model checkpoint.\n",
    "- We use the `AdamW` optimizer, which is good for training transformers.\n",
    "\n",
    "```python\n",
    "train_losses = []   # List to store training loss over time\n",
    "val_losses = []     # List to store validation loss over time\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Training Loop\n",
    "\n",
    "```python\n",
    "for iteration in range(max_iters):   # Loop over training steps\n",
    "```\n",
    "\n",
    "- Loop from `0` to `999` (total 1000 iterations).\n",
    "\n",
    "```python\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "```\n",
    "\n",
    "- Every `eval_interval` steps (e.g. every 10 steps), or on the final step:\n",
    "- We check how the model is performing.\n",
    "\n",
    "```python\n",
    "        losses = estimate_loss()  # Estimate the average train and val loss\n",
    "```\n",
    "\n",
    "- Calls the `estimate_loss()` function to evaluate how well the model is doing on both training and validation data.\n",
    "\n",
    "```python\n",
    "        print(\n",
    "            f\"step {iteration}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- Prints the current step and the train/validation loss (rounded to 4 decimal places).\n",
    "\n",
    "```python\n",
    "        train_losses.append(losses['train'])  # Save the train loss to list\n",
    "        val_losses.append(losses['val'])      # Save the val loss to list\n",
    "```\n",
    "\n",
    "- Appends the losses to their respective lists so you can track them later (e.g. plot a graph).\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Model Training Step\n",
    "\n",
    "```python\n",
    "    x_batch, y_batch = get_batch('train')  # Get a batch of training data\n",
    "```\n",
    "\n",
    "- Retrieves a random batch of input/output pairs for training.\n",
    "\n",
    "```python\n",
    "    logits, loss = model(x_batch, y_batch)  # Forward pass: predict and calculate loss\n",
    "```\n",
    "\n",
    "- Feeds the input batch through the model.\n",
    "- Gets predictions (`logits`) and calculates the loss.\n",
    "\n",
    "```python\n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear previous gradients\n",
    "```\n",
    "\n",
    "- Clears old gradients before computing new ones.\n",
    "\n",
    "```python\n",
    "    loss.backward()  # Backward pass: compute gradients\n",
    "```\n",
    "\n",
    "- Calculates gradients by backpropagation.\n",
    "\n",
    "```python\n",
    "    optimizer.step()  # Update model weights\n",
    "```\n",
    "\n",
    "- Applies the gradients to update the model parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Save Checkpoint\n",
    "\n",
    "```python\n",
    "    if iteration % save_interval == 0:\n",
    "```\n",
    "\n",
    "- Every `save_interval` steps (e.g. every 100 steps):\n",
    "\n",
    "```python\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=iteration,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_1/checkpoint_{iteration}.pth\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- Saves the model, optimizer state, current step (`iteration`), and current loss to a checkpoint file.\n",
    "- File is named like `checkpoint_0.pth`, `checkpoint_100.pth`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "This training loop does the following:\n",
    "- Trains a language model for 1000 iterations.\n",
    "- Evaluates and prints loss every 10 steps.\n",
    "- Saves the model every 100 steps.\n",
    "- Tracks loss history for plotting or analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylim(0)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Plotting Training and Validation Loss\n",
    "\n",
    "This code visualizes how the training and validation losses change over time during model training.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt  # Import the plotting library\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 5))     # Create a figure of size 10x5 inches\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.plot(train_losses, label=\"Train Loss\")        # Plot training loss curve\n",
    "plt.plot(val_losses, label=\"Validation Loss\")     # Plot validation loss curve\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.xlabel(\"Evaluation Step\")      # Label for the x-axis\n",
    "plt.ylim(0)                       # Set the y-axis to start from 0 (no negative loss)\n",
    "plt.ylabel(\"Loss\")                # Label for the y-axis\n",
    "plt.title(\"Training and Validation Loss Over Time\")  # Chart title\n",
    "plt.legend()                     # Show legend to distinguish train/val curves\n",
    "plt.grid()                       # Add grid lines for better readability\n",
    "plt.show()                       # Display the plot\n",
    "```\n",
    "\n",
    "### What You‚Äôll See\n",
    "\n",
    "- A line graph with two curves:\n",
    "  - **Train Loss** decreasing as the model learns.\n",
    "  - **Validation Loss** showing how well the model performs on unseen data.\n",
    "- The x-axis shows evaluation steps (every time loss was calculated).\n",
    "- The y-axis shows the loss values.\n",
    "\n",
    "This helps you visually check if the model is improving and if it‚Äôs overfitting or underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(\"Salam labas\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Generating Text with the Model\n",
    "\n",
    "This code takes an input text, processes it, and uses the model to generate new text based on that input.\n",
    "\n",
    "```python\n",
    "input_tokens = tokenizer.encode(\"Salam labas\")\n",
    "```\n",
    "- Converts the input text `\"Salam labas\"` into a list of token IDs using the tokenizer.\n",
    "\n",
    "```python\n",
    "input_tokens = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "```\n",
    "- Converts the list of tokens into a PyTorch tensor.\n",
    "- `.unsqueeze(0)` adds a batch dimension, making it shape `(1, sequence_length)`.\n",
    "- Moves the tensor to the device (CPU or GPU) where the model is located.\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "- Sets the model to evaluation mode (disables dropout and other training behaviors).\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=50)\n",
    "```\n",
    "- Disables gradient computation (since we‚Äôre only generating text, not training).\n",
    "- Uses the model‚Äôs `.generate()` method to produce up to 50 new tokens based on the input.\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(output[0].tolist()))\n",
    "```\n",
    "- Converts the generated token IDs back to text.\n",
    "- Prints the generated text output.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "This snippet generates text starting from `\"Salam labas\"` and continues for 50 tokens, showing the model‚Äôs prediction in natural language.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
