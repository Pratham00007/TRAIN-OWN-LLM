{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2835c3bd",
   "metadata": {},
   "source": [
    "# Load the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c181b",
   "metadata": {},
   "source": [
    "this is better than 2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951fe191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0521b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import BasicTokenizer\n",
    "\n",
    "tokenizer=BasicTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/my_tokenizer.model\")\n",
    "\n",
    "def get_vocab_size(tokenizer:BasicTokenizer)->int:\n",
    "    vocab=tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a52fd",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fb533d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2067f3f6dd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5477612",
   "metadata": {},
   "source": [
    "\n",
    "### üß† Explanation: Setting up Random Seed in PyTorch\n",
    "\n",
    "#### 1Ô∏è‚É£ Importing the PyTorch Library\n",
    "```python\n",
    "import torch\n",
    "```\n",
    "\n",
    "* **`torch`** is the main PyTorch library used for building and training neural networks.\n",
    "* It provides tools for:\n",
    "\n",
    "  * Creating tensors (multi-dimensional arrays)\n",
    "  * Performing mathematical operations\n",
    "  * Running models on CPU or GPU\n",
    "  * Handling deep learning workflows like automatic differentiation and optimization.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Setting a Manual Seed\n",
    "\n",
    "```python\n",
    "torch.manual_seed(3647)\n",
    "```\n",
    "\n",
    "* **What this does:**\n",
    "  It sets the *random seed* for PyTorch‚Äôs random number generator.\n",
    "  Random numbers are used in many parts of AI/ML ‚Äî for example, when:\n",
    "\n",
    "  * Initializing model weights\n",
    "  * Shuffling training data\n",
    "  * Performing random augmentations\n",
    "\n",
    "* **Why set a seed?**\n",
    "  To make your results **reproducible**.\n",
    "  Without fixing the seed, every time you run the code, random operations might produce different results.\n",
    "\n",
    "* **Parameter Explanation:**\n",
    "\n",
    "  * `3647` ‚Üí This is just a random integer chosen as the *seed value*.\n",
    "    You can use any integer (like 0, 42, or 9999).\n",
    "    Using the same seed ensures you‚Äôll get the same random numbers each time you run the program.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words:**\n",
    "This code imports PyTorch and fixes the random behavior so that your results stay the same every time you re-run the notebook.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8527e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.795338 M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size = 256\n",
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fe846",
   "metadata": {},
   "source": [
    "# Data prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8733f",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08655f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../output/combined_text.txt\",\"r\",encoding='utf-8') as f:\n",
    "    text_sequence=f.read()\n",
    "\n",
    "encoded_text_sequence=tokenizer.encode(text_sequence)\n",
    "len(encoded_text_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb7cd8",
   "metadata": {},
   "source": [
    "### 2. Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dceeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoded_text_sequence,dtype=torch.long)\n",
    "split_index=int(0.9*len(data))\n",
    "train_data=data[:split_index]\n",
    "val_data =data[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da648fb",
   "metadata": {},
   "source": [
    "### 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c3fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,data: torch.Tensor,block_size:int) -> None:\n",
    "        if len(data) <= block_size:\n",
    "            raise ValueError(\n",
    "                f\"The length of the data ({len(data)}) must be grater than the block_size ({block_size}).\"\n",
    "            )\n",
    "        \n",
    "        self.data=data\n",
    "        self.block_size=block_size\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, index:int)->Tuple[torch.Tensor,torch.Tensor]:\n",
    "        x=self.data[index : index + self.block_size]\n",
    "        y=self.data[index + 1: index + self.block_size+1]\n",
    "        return x,y\n",
    "    \n",
    "\n",
    "def get_dataloaders(\n",
    "    train_data: torch.Tensor,\n",
    "    val_data: torch.Tensor,\n",
    "    block_size: int,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train_dataset = TextDataset(train_data.to(device), block_size)\n",
    "    val_dataset = TextDataset(val_data.to(device), block_size)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4b951",
   "metadata": {},
   "source": [
    "\n",
    "### üß† Explanation: Creating a Custom Text Dataset and Data Loaders in PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ Importing Required Modules\n",
    "```python\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "```\n",
    "\n",
    "* **`typing.Tuple`** ‚Üí Used for type hints.\n",
    "  It tells readers (and some tools) that a function will return a tuple ‚Äî for example, `(x, y)`.\n",
    "* **`torch.utils.data.Dataset`** ‚Üí Base class for creating custom datasets in PyTorch.\n",
    "* **`torch.utils.data.DataLoader`** ‚Üí Helper class that loads data from a `Dataset` in **batches**, optionally **shuffling** them for training.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Creating the Custom Dataset Class\n",
    "\n",
    "```python\n",
    "class TextDataset(Dataset):\n",
    "```\n",
    "\n",
    "* You‚Äôre defining a new class `TextDataset` that inherits from PyTorch‚Äôs `Dataset`.\n",
    "* This lets you store and organize your data in a format that PyTorch can easily use.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Initializing the Dataset\n",
    "\n",
    "```python\n",
    "def __init__(self, data: torch.Tensor, block_size: int) -> None:\n",
    "```\n",
    "\n",
    "* **`__init__`**: This is the constructor that runs when you create a `TextDataset` object.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `data`: The text data converted into a tensor of numbers (each number could represent a token or character).\n",
    "  * `block_size`: The number of tokens in each input sequence (like the length of a sentence chunk).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ Checking Data Length\n",
    "\n",
    "```python\n",
    "if len(data) <= block_size:\n",
    "    raise ValueError(\n",
    "        f\"The length of the data ({len(data)}) must be greater than the block_size ({block_size}).\"\n",
    "    )\n",
    "```\n",
    "\n",
    "* Ensures the dataset is large enough to form at least one valid input-target pair.\n",
    "* If not, it raises an error message.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ Storing Variables\n",
    "\n",
    "```python\n",
    "self.data = data\n",
    "self.block_size = block_size\n",
    "```\n",
    "\n",
    "* Saves both values for later use in other functions of this class.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6Ô∏è‚É£ Getting the Length of the Dataset\n",
    "\n",
    "```python\n",
    "def __len__(self) -> int:\n",
    "    return len(self.data) - self.block_size\n",
    "```\n",
    "\n",
    "* This tells PyTorch **how many samples** your dataset contains.\n",
    "* Each sample is made up of:\n",
    "\n",
    "  * An input (`x`) of length `block_size`\n",
    "  * A target (`y`) of length `block_size`\n",
    "* So, the last few elements of `data` can‚Äôt form a full block, which is why we subtract `block_size`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7Ô∏è‚É£ Getting an Item by Index\n",
    "\n",
    "```python\n",
    "def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    x = self.data[index : index + self.block_size]\n",
    "    y = self.data[index + 1 : index + self.block_size + 1]\n",
    "    return x, y\n",
    "```\n",
    "\n",
    "* **What happens here:**\n",
    "  For each `index`, we create:\n",
    "\n",
    "  * **`x` (input)** ‚Üí A slice of data of size `block_size`\n",
    "  * **`y` (target/output)** ‚Üí The same slice but shifted by one position to the right\n",
    "* Example:\n",
    "  If `data = [1, 2, 3, 4, 5]` and `block_size = 3`, then:\n",
    "\n",
    "  * At `index = 0`:\n",
    "\n",
    "    * `x = [1, 2, 3]`\n",
    "    * `y = [2, 3, 4]`\n",
    "      This helps the model learn **the next token prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Function: Creating DataLoaders\n",
    "\n",
    "```python\n",
    "def get_dataloaders(\n",
    "    train_data: torch.Tensor,\n",
    "    val_data: torch.Tensor,\n",
    "    block_size: int,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "```\n",
    "\n",
    "This function creates two **DataLoaders** ‚Äî one for **training** and one for **validation**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Create Datasets\n",
    "\n",
    "```python\n",
    "train_dataset = TextDataset(train_data.to(device), block_size)\n",
    "val_dataset = TextDataset(val_data.to(device), block_size)\n",
    "```\n",
    "\n",
    "* Converts the data to the chosen **device** (CPU or GPU).\n",
    "* Creates `TextDataset` objects for both training and validation data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Wrap Datasets into DataLoaders\n",
    "\n",
    "```python\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "```\n",
    "\n",
    "* **`batch_size`** ‚Üí How many samples to load at once.\n",
    "  Example: If `batch_size = 32`, each iteration gives 32 input‚Äìtarget pairs.\n",
    "* **`shuffle=True`** ‚Üí Randomizes the order of samples in each epoch (good for training).\n",
    "* **`shuffle=False`** ‚Üí Keeps order consistent for validation/testing.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Return Both Loaders\n",
    "\n",
    "```python\n",
    "return train_loader, val_loader\n",
    "```\n",
    "\n",
    "* Returns a tuple containing:\n",
    "\n",
    "  * `train_loader`: for model training\n",
    "  * `val_loader`: for model evaluation\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words:**\n",
    "This code:\n",
    "\n",
    "1. Defines how to slice text data into small training examples (`TextDataset`).\n",
    "2. Packs them into batches with PyTorch‚Äôs `DataLoader`.\n",
    "3. Makes it easy to feed text data into a neural network for training and validation.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd74ddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The length of the data (150) must be grater than the block_size (256).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader,val_loader\u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m x,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[0;32m     10\u001b[0m x\u001b[38;5;241m.\u001b[39mshape,y\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m, in \u001b[0;36mget_dataloaders\u001b[1;34m(train_data, val_data, block_size, batch_size, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataloaders\u001b[39m(\n\u001b[0;32m     24\u001b[0m     train_data: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     25\u001b[0m     val_data: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     device: torch\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DataLoader, DataLoader]:\n\u001b[1;32m---> 30\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m TextDataset(val_data\u001b[38;5;241m.\u001b[39mto(device), block_size)\n\u001b[0;32m     33\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     34\u001b[0m         train_dataset,\n\u001b[0;32m     35\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     36\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[1;34m(self, data, block_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,data: torch\u001b[38;5;241m.\u001b[39mTensor,block_size:\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m block_size:\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe length of the data (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be grater than the block_size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m         )\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m=\u001b[39mdata\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size\u001b[38;5;241m=\u001b[39mblock_size\n",
      "\u001b[1;31mValueError\u001b[0m: The length of the data (150) must be grater than the block_size (256)."
     ]
    }
   ],
   "source": [
    "train_loader,val_loader= get_dataloaders(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "x,y=next(iter(train_loader))\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857b0ee",
   "metadata": {},
   "source": [
    "\n",
    "### üß† Explanation: Loading and Inspecting a Batch of Training Data\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ Creating Training and Validation DataLoaders\n",
    "```python\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "```\n",
    "\n",
    "* This line **calls** the `get_dataloaders()` function (which we defined earlier).\n",
    "* It prepares two **DataLoaders** ‚Äî one for training and one for validation.\n",
    "\n",
    "Let‚Äôs break down each argument üëá\n",
    "\n",
    "| Parameter        | Meaning                                                                       |\n",
    "| ---------------- | ----------------------------------------------------------------------------- |\n",
    "| **`train_data`** | Tensor containing all training data (numerical representation of text).       |\n",
    "| **`val_data`**   | Tensor for validation data (used to check model performance).                 |\n",
    "| **`block_size`** | The number of tokens in one input sequence (length of each training example). |\n",
    "| **`batch_size`** | Number of such input‚Äìoutput pairs processed at once during training.          |\n",
    "| **`device`**     | Tells PyTorch whether to store data on the **CPU** or **GPU**.                |\n",
    "\n",
    "* The function returns two loaders:\n",
    "\n",
    "  * **`train_loader`** ‚Üí Randomized batches for training.\n",
    "  * **`val_loader`** ‚Üí Sequential batches for validation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Getting One Batch from the Training DataLoader\n",
    "\n",
    "```python\n",
    "x, y = next(iter(train_loader))\n",
    "```\n",
    "\n",
    "* **`iter(train_loader)`** ‚Üí Converts the DataLoader into an **iterator**, meaning we can go through it batch by batch.\n",
    "* **`next(...)`** ‚Üí Gets the **first batch** from that iterator.\n",
    "* Each batch returns a **tuple**:\n",
    "\n",
    "  * **`x`** ‚Üí Batch of input sequences (each of length `block_size`).\n",
    "  * **`y`** ‚Üí Batch of target sequences (the same as `x` but shifted by one position).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Checking the Shape of Batches\n",
    "\n",
    "```python\n",
    "x.shape, y.shape\n",
    "```\n",
    "\n",
    "* This prints the **dimensions (shape)** of both tensors.\n",
    "\n",
    "* The expected shape is usually:\n",
    "\n",
    "  ```\n",
    "  (batch_size, block_size)\n",
    "  ```\n",
    "\n",
    "* Example:\n",
    "  If `batch_size = 64` and `block_size = 8`, then:\n",
    "\n",
    "  ```\n",
    "  x.shape = torch.Size([64, 8])\n",
    "  y.shape = torch.Size([64, 8])\n",
    "  ```\n",
    "\n",
    "  ‚Üí meaning: **64 examples per batch**, each with **8 tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words:**\n",
    "This code takes your text data, divides it into mini-groups (batches) of input and target sequences, and shows the size of one batch that will be fed into the model during training.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c550a",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "@torch.no_grad()\n",
    "def estimata_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader:DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    eval_iters: int \n",
    ")-> Dict[str,float]:\n",
    "    output={}\n",
    "    model.eval()\n",
    "\n",
    "    for split , loader in [('train',train_loader),('val','val_loader')]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i ,(x,y) in enumerate(loader):\n",
    "            if i>= eval_iters:\n",
    "                break\n",
    "            with torch.no_grad():\n",
    "                _,loss=model(x,y)\n",
    "            losses[i]=loss.item()\n",
    "        output[split]=losses.mean().item()\n",
    "    \n",
    "    model.train()\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10032f6",
   "metadata": {},
   "source": [
    "\n",
    "### üß† Explanation: Estimating Loss for Training and Validation Sets\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ Importing Required Module\n",
    "```python\n",
    "from typing import Dict\n",
    "```\n",
    "\n",
    "* **`Dict`** from the `typing` module is used for **type hints**.\n",
    "* It indicates that a function will return a **dictionary** ‚Äî here, `{ \"train\": value, \"val\": value }`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Disabling Gradient Calculation\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "```\n",
    "\n",
    "* This **decorator** tells PyTorch **not to track gradients** during the function call.\n",
    "* Why?\n",
    "\n",
    "  * When we are **evaluating** (not training), we don‚Äôt need gradients.\n",
    "  * It **saves memory** and makes computations **faster**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Defining the Function\n",
    "\n",
    "```python\n",
    "def estimata_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    eval_iters: int\n",
    ") -> Dict[str, float]:\n",
    "```\n",
    "\n",
    "* This function computes the **average loss** for both training and validation datasets over a few batches.\n",
    "\n",
    "| Parameter          | Type               | Description                                               |\n",
    "| ------------------ | ------------------ | --------------------------------------------------------- |\n",
    "| **`model`**        | `torch.nn.Module`  | The neural network to evaluate.                           |\n",
    "| **`train_loader`** | `DataLoader`       | DataLoader providing training data batches.               |\n",
    "| **`val_loader`**   | `DataLoader`       | DataLoader providing validation data batches.             |\n",
    "| **`eval_iters`**   | `int`              | Number of batches to use for estimating the average loss. |\n",
    "| **Returns**        | `Dict[str, float]` | Dictionary with average losses for `\"train\"` and `\"val\"`. |\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ Creating an Empty Output Dictionary\n",
    "\n",
    "```python\n",
    "output = {}\n",
    "```\n",
    "\n",
    "* This will store the results like:\n",
    "\n",
    "  ```python\n",
    "  {'train': 0.45, 'val': 0.50}\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ Setting the Model to Evaluation Mode\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "* Switches the model to **evaluation mode**, which:\n",
    "\n",
    "  * Turns off dropout layers.\n",
    "  * Disables batch normalization updates.\n",
    "* This ensures consistent behavior while testing or validating.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6Ô∏è‚É£ Iterating Through Both Loaders\n",
    "\n",
    "```python\n",
    "for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "```\n",
    "\n",
    "* Loops over **two datasets**: one for training and one for validation.\n",
    "* Each iteration sets:\n",
    "\n",
    "  * `split` ‚Üí either `\"train\"` or `\"val\"`.\n",
    "  * `loader` ‚Üí the corresponding DataLoader.\n",
    "\n",
    "‚ö†Ô∏è **Note:**\n",
    "In your code, `'val','val_loader'` has quotes around `val_loader`, which makes it a string ‚Äî that‚Äôs a bug.\n",
    "It should be:\n",
    "\n",
    "```python\n",
    "for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7Ô∏è‚É£ Initializing a Tensor for Loss Storage\n",
    "\n",
    "```python\n",
    "losses = torch.zeros(eval_iters)\n",
    "```\n",
    "\n",
    "* Creates a tensor (array) to store the loss value from each batch.\n",
    "* Size = number of evaluation iterations (`eval_iters`).\n",
    "\n",
    "---\n",
    "\n",
    "#### 8Ô∏è‚É£ Looping Through Batches\n",
    "\n",
    "```python\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    if i >= eval_iters:\n",
    "        break\n",
    "```\n",
    "\n",
    "* Iterates through the DataLoader, one batch at a time.\n",
    "* **`i`** = batch index, **`(x, y)`** = input and target tensors.\n",
    "* Stops once it has processed `eval_iters` batches.\n",
    "\n",
    "---\n",
    "\n",
    "#### 9Ô∏è‚É£ Calculating Loss (Without Gradients)\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    _, loss = model(x, y)\n",
    "losses[i] = loss.item()\n",
    "```\n",
    "\n",
    "* **`with torch.no_grad()`** ‚Üí Double safety to ensure no gradient tracking.\n",
    "* Calls the model with input `x` and target `y`.\n",
    "\n",
    "  * The model returns a tuple like `(logits, loss)`.\n",
    "  * We only need the `loss` here.\n",
    "* **`.item()`** converts a 1-element tensor to a regular Python number and stores it in `losses[i]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîü Calculating the Average Loss\n",
    "\n",
    "```python\n",
    "output[split] = losses.mean().item()\n",
    "```\n",
    "\n",
    "* Computes the **mean of all stored losses** for that dataset (`train` or `val`).\n",
    "* Stores it in the `output` dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "#### 11Ô∏è‚É£ Switching Model Back to Training Mode\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "```\n",
    "\n",
    "* Puts the model back into **training mode** (enables dropout, etc.)\n",
    "* Important because later training steps require it.\n",
    "\n",
    "---\n",
    "\n",
    "#### 12Ô∏è‚É£ Returning the Result\n",
    "\n",
    "```python\n",
    "return output\n",
    "```\n",
    "\n",
    "* Returns a dictionary like:\n",
    "\n",
    "  ```python\n",
    "  {'train': 0.42, 'val': 0.50}\n",
    "  ```\n",
    "\n",
    "  which contains the average loss for both datasets.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words:**\n",
    "This function quickly checks how well your model is performing on both **training** and **validation** data ‚Äî without updating weights ‚Äî and returns their average loss values.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7f42a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPTLanguageModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_checkout\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m         mode:\u001b[43mGPTLanguageModel\u001b[49m,\n\u001b[0;32m      3\u001b[0m         optimizer:torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m      4\u001b[0m         epoch:\u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m      5\u001b[0m         loss:\u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m      6\u001b[0m         file_path:\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     checkpoint\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\n\u001b[0;32m     14\u001b[0m     }\n\u001b[0;32m     15\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(checkpoint,file_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GPTLanguageModel' is not defined"
     ]
    }
   ],
   "source": [
    "def save_checkout(\n",
    "        mode:GPTLanguageModel,\n",
    "        optimizer:torch.optim.Optimizer,\n",
    "        epoch:int,\n",
    "        loss:float,\n",
    "        file_path:str=\"checkpoint.pth\"\n",
    "\n",
    ")-> None:\n",
    "    checkpoint={\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint,file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0ddac",
   "metadata": {},
   "source": [
    "\n",
    "### üíæ Explanation: Saving a Model Checkpoint in PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ Defining a Function to Save Model State\n",
    "```python\n",
    "def save_checkout(\n",
    "        model: GPTLanguageModel,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epoch: int,\n",
    "        loss: float,\n",
    "        file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "```\n",
    "\n",
    "Let‚Äôs break down every part üëá\n",
    "\n",
    "##### üß© Function Purpose\n",
    "\n",
    "* This function **saves the training progress** ‚Äî including model weights, optimizer state, current epoch, and loss ‚Äî into a file called a **checkpoint**.\n",
    "* Checkpoints let you:\n",
    "\n",
    "  * Stop training midway and **resume later** without losing progress.\n",
    "  * **Restore** a trained model anytime for testing or inference.\n",
    "  * **Avoid retraining** the model from scratch if something goes wrong.\n",
    "\n",
    "##### üß† Parameters Explained\n",
    "\n",
    "| Parameter       | Type                    | Description                                                                                        |\n",
    "| --------------- | ----------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| **`model`**     | `GPTLanguageModel`      | The neural network whose weights (parameters) you want to save.                                    |\n",
    "| **`optimizer`** | `torch.optim.Optimizer` | The optimizer (like Adam, SGD) that updates model parameters.                                      |\n",
    "| **`epoch`**     | `int`                   | The current epoch number ‚Äî i.e., how many complete passes through the training data have occurred. |\n",
    "| **`loss`**      | `float`                 | The loss value at this stage of training, useful for tracking performance.                         |\n",
    "| **`file_path`** | `str`                   | Path and filename where checkpoint will be saved. Default: `\"checkpoint.pth\"`.                     |\n",
    "\n",
    "##### üß© Return Type\n",
    "\n",
    "* **`-> None`** ‚Üí The function doesn‚Äôt return anything; it just saves data to disk.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Creating the Checkpoint Dictionary\n",
    "\n",
    "```python\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "}\n",
    "```\n",
    "\n",
    "##### üí° What is a ‚Äúcheckpoint‚Äù?\n",
    "\n",
    "* A **checkpoint** is a dictionary that stores everything needed to **rebuild your model** and **continue training later**.\n",
    "\n",
    "##### üß± Explanation of Each Key‚ÄìValue Pair\n",
    "\n",
    "| Key                      | Value                    | Description                                                                        |\n",
    "| ------------------------ | ------------------------ | ---------------------------------------------------------------------------------- |\n",
    "| `'epoch'`                | `epoch`                  | Saves the current epoch number, so you know from where to continue training later. |\n",
    "| `'model_state_dict'`     | `model.state_dict()`     | Saves all learnable parameters (weights and biases) of the model.                  |\n",
    "| `'optimizer_state_dict'` | `optimizer.state_dict()` | Saves optimizer settings (like learning rate, momentum, and running averages).     |\n",
    "| `'loss'`                 | `loss`                   | Records the last loss value for reference when resuming training.                  |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç Deep Dive: What is `state_dict()`?\n",
    "\n",
    "Both models and optimizers in PyTorch maintain internal **state dictionaries** ‚Äî i.e., Python dictionaries containing their parameters.\n",
    "\n",
    "```python\n",
    "model.state_dict()\n",
    "```\n",
    "\n",
    "* Returns something like:\n",
    "\n",
    "  ```python\n",
    "  {\n",
    "      'layer1.weight': tensor([...]),\n",
    "      'layer1.bias': tensor([...]),\n",
    "      'layer2.weight': tensor([...]),\n",
    "      ...\n",
    "  }\n",
    "  ```\n",
    "* Each entry represents one **parameter tensor** (weights or biases).\n",
    "\n",
    "```python\n",
    "optimizer.state_dict()\n",
    "```\n",
    "\n",
    "* Stores internal data such as:\n",
    "\n",
    "  * The **current learning rate**.\n",
    "  * **Momentum** buffers (used in SGD, Adam, etc.).\n",
    "  * **Parameter groups** (if different parts of the model use different settings).\n",
    "* This ensures that when you **reload the checkpoint**, the optimizer resumes exactly from where it left off (so training dynamics continue smoothly).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Saving the Checkpoint to Disk\n",
    "\n",
    "```python\n",
    "torch.save(checkpoint, file_path)\n",
    "```\n",
    "\n",
    "##### üíæ What happens here\n",
    "\n",
    "* **`torch.save()`** serializes the Python dictionary (`checkpoint`) and saves it as a binary file at the specified path.\n",
    "* The file extension `.pth` or `.pt` is the common PyTorch format (both work the same).\n",
    "\n",
    "##### ‚öôÔ∏è Internal process:\n",
    "\n",
    "* PyTorch uses **`pickle`** (Python‚Äôs object serialization library) under the hood to convert tensors and metadata into a storable format.\n",
    "* This allows later restoration using:\n",
    "\n",
    "  ```python\n",
    "  checkpoint = torch.load(\"checkpoint.pth\")\n",
    "  ```\n",
    "\n",
    "##### üß† Why it‚Äôs useful:\n",
    "\n",
    "* You can later **resume training** like this:\n",
    "\n",
    "  ```python\n",
    "  checkpoint = torch.load(\"checkpoint.pth\")\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch = checkpoint['epoch']\n",
    "  loss = checkpoint['loss']\n",
    "  ```\n",
    "\n",
    "  ‚Üí This restores the model and optimizer **exactly** to their saved states.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ In simple words:\n",
    "\n",
    "This function takes a snapshot of your model‚Äôs current state ‚Äî including:\n",
    "\n",
    "* its learned parameters,\n",
    "* optimizer progress,\n",
    "* current training step (epoch),\n",
    "* and the latest loss value ‚Äî\n",
    "\n",
    "and **saves it all to a file** so that you can continue training later or deploy the model without starting over.\n",
    "\n",
    "---\n",
    "\n",
    "üß© **Example Usage**\n",
    "\n",
    "```python\n",
    "save_checkout(model, optimizer, epoch=10, loss=0.032, file_path=\"gpt_checkpoint.pth\")\n",
    "```\n",
    "\n",
    "‚û°Ô∏è This will save a file `gpt_checkpoint.pth` containing everything needed to restore the model at **epoch 10** with a **loss of 0.032**.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec46b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (692356186.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 20\u001b[1;36m\u001b[0m\n\u001b[1;33m    # Evaluation\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "max_iters=1\n",
    "eval_interval=100\n",
    "eval_iters = 200\n",
    "learning_rate=3e-4\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "train_loader , val_loader = get_dataloaders(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    for batch_idx, (x_batch,y_batch) in enumerate(train_loader):\n",
    "        # Evaluation\n",
    "        if batch_idx % eval_interval ==0 or batch_idx == len (train_loader) -1:\n",
    "            losses = estimata_loss(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                eval_iters=min(eval_iters,len(val_loader))\n",
    "\n",
    "            )\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "\n",
    "            print(\n",
    "                f\"iteration {iteration} / step {batch_idx}: \"\n",
    "                f\"train loss {losses['train']:.4f}, \"\n",
    "                f\"val loss {losses['val']:.4f}\"\n",
    "\n",
    "            )\n",
    "\n",
    "            # training step\n",
    "            logits,loss=model(x_batch,y_batch)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # save checkpoint\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=iteration,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_4/checkpoint_{iteration}.pth\"\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b784cac",
   "metadata": {},
   "source": [
    "\n",
    "### üß† Explanation: Full Training Loop for GPT Language Model\n",
    "\n",
    "This code defines how the model is **trained**, **evaluated**, and **saved** after every few steps.  \n",
    "It combines all the building blocks we defined earlier ‚Äî dataset, dataloaders, optimizer, and checkpoint functions.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ Setting Hyperparameters\n",
    "```python\n",
    "max_iters = 1\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "```\n",
    "\n",
    "| Variable            | Meaning                                                                                               | Description |\n",
    "| ------------------- | ----------------------------------------------------------------------------------------------------- | ----------- |\n",
    "| **`max_iters`**     | Total number of epochs (complete passes over the dataset). Here only `1` for quick test.              |             |\n",
    "| **`eval_interval`** | How often (in batches) to evaluate model performance during training.                                 |             |\n",
    "| **`eval_iters`**    | Number of batches to use for calculating average loss during evaluation.                              |             |\n",
    "| **`learning_rate`** | How big each weight update step is ‚Äî smaller = slower but more stable learning. Here `3e-4` = 0.0003. |             |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Creating the Optimizer\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "* **`AdamW`** is an improved version of the Adam optimizer that includes **weight decay**, which helps regularize the model and prevent overfitting.\n",
    "* **`model.parameters()`** ‚Üí Gives all the trainable weights (tensors) in the model.\n",
    "* **`lr=learning_rate`** ‚Üí Sets the learning rate to `0.0003`.\n",
    "\n",
    "üîç Internally:\n",
    "\n",
    "* During training, the optimizer:\n",
    "\n",
    "  1. Reads gradients from each parameter (`param.grad`).\n",
    "  2. Updates each parameter slightly to reduce the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Getting the DataLoaders\n",
    "\n",
    "```python\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "```\n",
    "\n",
    "* Loads the training and validation datasets into batches.\n",
    "* Each batch gives `(x_batch, y_batch)` tensors for the model to train on.\n",
    "* `block_size` = how long each input sequence is.\n",
    "* `batch_size` = how many such sequences per batch.\n",
    "* `device` = CPU or GPU.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ Lists to Store Loss Values\n",
    "\n",
    "```python\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "```\n",
    "\n",
    "* Empty lists used to **track loss values** over time so you can later plot training progress.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ Starting the Training Loop\n",
    "\n",
    "```python\n",
    "for iteration in range(max_iters):\n",
    "```\n",
    "\n",
    "* The **outer loop** runs for a number of epochs (`max_iters` times).\n",
    "* Each ‚Äúiteration‚Äù here corresponds to one epoch (a complete pass through the training set).\n",
    "\n",
    "---\n",
    "\n",
    "#### 6Ô∏è‚É£ Inner Loop: Going Through Each Batch\n",
    "\n",
    "```python\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "```\n",
    "\n",
    "* Loops through the batches of training data.\n",
    "* **`batch_idx`** ‚Üí Index of the batch (0, 1, 2, ‚Ä¶).\n",
    "* **`x_batch`** ‚Üí Input text sequences.\n",
    "* **`y_batch`** ‚Üí Target text sequences (shifted by one token).\n",
    "\n",
    "---\n",
    "\n",
    "#### 7Ô∏è‚É£ Performing Evaluation at Intervals\n",
    "\n",
    "```python\n",
    "if batch_idx % eval_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "```\n",
    "\n",
    "* Runs evaluation:\n",
    "\n",
    "  * Every `eval_interval` batches (e.g., every 100 steps).\n",
    "  * OR at the **end** of the epoch (last batch).\n",
    "\n",
    "---\n",
    "\n",
    "#### 8Ô∏è‚É£ Estimate Loss for Train and Validation\n",
    "\n",
    "```python\n",
    "losses = estimata_loss(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    eval_iters=min(eval_iters, len(val_loader))\n",
    ")\n",
    "```\n",
    "\n",
    "* Calls the **`estimata_loss()`** function we defined earlier.\n",
    "* Evaluates both **train** and **validation** average loss.\n",
    "* **`eval_iters=min(eval_iters, len(val_loader))`** ‚Üí Ensures we don‚Äôt exceed available batches.\n",
    "\n",
    "The result looks like:\n",
    "\n",
    "```python\n",
    "{'train': 0.4251, 'val': 0.5023}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 9Ô∏è‚É£ Record the Losses\n",
    "\n",
    "```python\n",
    "train_losses.append(losses['train'])\n",
    "val_losses.append(losses['val'])\n",
    "```\n",
    "\n",
    "* Adds current losses to the tracking lists for future plotting or analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîü Print Progress to Console\n",
    "\n",
    "```python\n",
    "print(\n",
    "    f\"iteration {iteration} / step {batch_idx}: \"\n",
    "    f\"train loss {losses['train']:.4f}, \"\n",
    "    f\"val loss {losses['val']:.4f}\"\n",
    ")\n",
    "```\n",
    "\n",
    "* Displays the current progress in human-readable form:\n",
    "\n",
    "  ```\n",
    "  iteration 0 / step 100: train loss 0.4213, val loss 0.5072\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 11Ô∏è‚É£ Forward Pass (Training Step)\n",
    "\n",
    "```python\n",
    "logits, loss = model(x_batch, y_batch)\n",
    "```\n",
    "\n",
    "* **`model(x_batch, y_batch)`** runs the input through the neural network.\n",
    "* Returns:\n",
    "\n",
    "  * **`logits`** ‚Üí Model‚Äôs predicted outputs before applying softmax.\n",
    "  * **`loss`** ‚Üí Computed difference between predictions and actual targets.\n",
    "\n",
    "---\n",
    "\n",
    "#### 12Ô∏è‚É£ Resetting Gradients\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "```\n",
    "\n",
    "* Before computing new gradients, old ones must be cleared.\n",
    "* If not cleared, PyTorch accumulates them (adds new to old).\n",
    "* **`set_to_none=True`** sets gradients to `None` instead of `0`, which saves memory and improves speed.\n",
    "\n",
    "---\n",
    "\n",
    "#### 13Ô∏è‚É£ Backpropagation (Compute Gradients)\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "* This computes the **gradients** of the loss with respect to each model parameter.\n",
    "* PyTorch automatically calculates these using the **autograd** engine.\n",
    "\n",
    "---\n",
    "\n",
    "#### 14Ô∏è‚É£ Update Model Parameters\n",
    "\n",
    "```python\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "* The optimizer uses the computed gradients to **update model weights**.\n",
    "* Each weight moves slightly in the direction that **reduces loss**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 15Ô∏è‚É£ Save Model Checkpoint\n",
    "\n",
    "```python\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=iteration,\n",
    "    loss=loss.item(),\n",
    "    file_path=f\"../output/pre_training/run_4/checkpoint_{iteration}.pth\"\n",
    ")\n",
    "```\n",
    "\n",
    "* Saves the current model state to disk after each epoch.\n",
    "* **`loss.item()`** converts the tensor loss to a plain Python float.\n",
    "* The file path dynamically includes the iteration number so multiple checkpoints are saved separately:\n",
    "\n",
    "  ```\n",
    "  checkpoint_0.pth\n",
    "  checkpoint_1.pth\n",
    "  ...\n",
    "  ```\n",
    "* Each checkpoint contains:\n",
    "\n",
    "  * Model weights\n",
    "  * Optimizer state\n",
    "  * Current epoch\n",
    "  * Loss value\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Summary of the Whole Loop\n",
    "\n",
    "| Step  | Purpose                                                  |\n",
    "| ----- | -------------------------------------------------------- |\n",
    "| **1** | Load data and create model/optimizer                     |\n",
    "| **2** | Loop over epochs                                         |\n",
    "| **3** | Loop through mini-batches                                |\n",
    "| **4** | Periodically evaluate model (training + validation loss) |\n",
    "| **5** | Do forward pass, compute loss                            |\n",
    "| **6** | Backpropagate and update model weights                   |\n",
    "| **7** | Save checkpoint after each epoch                         |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words:**\n",
    "This code trains your model batch-by-batch, evaluates it at regular steps, updates the weights using backpropagation, and saves checkpoints so you can pause or continue training anytime.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Note:**\n",
    "Your function name here is written as `save_checkpoint`, but in your earlier code, it was defined as `save_checkout`.\n",
    "You should rename either one for consistency:\n",
    "\n",
    "```python\n",
    "def save_checkpoint(...):\n",
    "    ...\n",
    "```\n",
    "\n",
    "so that your training loop calls the correct function.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633105a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(val_losses, label=\"Validation Loss\", marker='o')\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf3a1a",
   "metadata": {},
   "source": [
    "\n",
    "### üìä Visualizing Training & Validation Loss Curves\n",
    "\n",
    "#### 1Ô∏è‚É£ Importing the Plotting Library\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "* **`matplotlib.pyplot`** is a popular Python library for creating plots and graphs.\n",
    "* We use it to **visually track model performance** over time.\n",
    "* The alias `plt` is just a shorthand (common convention).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Creating a New Figure\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 5))\n",
    "```\n",
    "\n",
    "* Creates a **new blank figure** (like a canvas) for plotting.\n",
    "* **`figsize=(10, 5)`** sets the **width = 10 inches** and **height = 5 inches** ‚Äî a rectangular graph layout.\n",
    "* This makes the plot more readable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Plotting the Training Loss\n",
    "\n",
    "```python\n",
    "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "```\n",
    "\n",
    "* **`train_losses`**: A Python list that stores the loss values recorded during training (from earlier loops).\n",
    "* **`label=\"Train Loss\"`** gives a name to this line for the legend.\n",
    "* **`marker='o'`** draws small circles at each data point ‚Äî helps visualize individual points clearly.\n",
    "* The line connects those points to show the trend of loss over time.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ Plotting the Validation Loss\n",
    "\n",
    "```python\n",
    "plt.plot(val_losses, label=\"Validation Loss\", marker='o')\n",
    "```\n",
    "\n",
    "* **`val_losses`**: Stores validation loss values measured during training.\n",
    "* This helps compare how well the model generalizes to unseen data.\n",
    "* Same styling: a line with circle markers and a legend label ‚ÄúValidation Loss‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ Adding Axis Labels\n",
    "\n",
    "```python\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "```\n",
    "\n",
    "* **`xlabel`** ‚Üí X-axis label: ‚ÄúEvaluation Step‚Äù\n",
    "\n",
    "  * Represents how many times we evaluated the model during training.\n",
    "* **`ylabel`** ‚Üí Y-axis label: ‚ÄúLoss‚Äù\n",
    "\n",
    "  * Shows the numerical loss value (lower = better model).\n",
    "\n",
    "---\n",
    "\n",
    "#### 6Ô∏è‚É£ Adding a Title\n",
    "\n",
    "```python\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "```\n",
    "\n",
    "* Adds a title at the top of the graph.\n",
    "* Clearly explains that the plot shows how loss changes as training progresses.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7Ô∏è‚É£ Adding Legend and Grid\n",
    "\n",
    "```python\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "```\n",
    "\n",
    "* **`plt.legend()`** ‚Üí Displays the labels (‚ÄúTrain Loss‚Äù, ‚ÄúValidation Loss‚Äù) in a small box to identify lines.\n",
    "* **`plt.grid()`** ‚Üí Adds light grid lines for easier reading of values.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8Ô∏è‚É£ Displaying the Plot\n",
    "\n",
    "```python\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* Renders and displays the complete plot.\n",
    "* Without this line, the plot may not appear in some environments (like Jupyter Notebooks).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "This code plots **two lines**:\n",
    "\n",
    "* üü© **Train Loss** ‚Äî how well the model fits the training data.\n",
    "* üü¶ **Validation Loss** ‚Äî how well the model performs on unseen data.\n",
    "\n",
    "If the graph shows both lines **decreasing steadily**, it means your model is learning properly.\n",
    "If validation loss starts increasing while training loss keeps decreasing ‚Äî your model may be **overfitting**.\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecdd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(\"Salam labas \")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6257ac7",
   "metadata": {},
   "source": [
    "\n",
    "### üß† Explanation: Generating Text from a Trained GPT Model\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ Tokenizing the Input Text\n",
    "```python\n",
    "input_tokens = tokenizer.encode(\"Salam labas \")\n",
    "```\n",
    "\n",
    "* **`tokenizer.encode()`** converts the raw text `\"Salam labas \"` into **numerical token IDs** that the model can understand.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  \"Salam labas \" ‚Üí [101, 1234, 5678, 0]\n",
    "  ```\n",
    "* Each number corresponds to a **word or subword token** in the model‚Äôs vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Converting Tokens to a Tensor\n",
    "\n",
    "```python\n",
    "input_tokens = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "```\n",
    "\n",
    "* **`torch.tensor(input_tokens, dtype=torch.long)`**\n",
    "  Converts the Python list of token IDs into a **PyTorch tensor** of type `long` (required for token IDs).\n",
    "\n",
    "* **`.unsqueeze(0)`**\n",
    "  Adds an extra dimension at the beginning to represent **batch size**.\n",
    "\n",
    "  * Model expects input shape: `(batch_size, sequence_length)`\n",
    "  * Example: `[101, 1234, 5678]` ‚Üí `[[101, 1234, 5678]]`\n",
    "\n",
    "* **`.to(device)`**\n",
    "  Moves the tensor to the **CPU or GPU** (`device`) so the model can process it.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Switching Model to Evaluation Mode\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "* Puts the model into **evaluation mode**:\n",
    "\n",
    "  * Turns off dropout layers.\n",
    "  * Ensures batch normalization layers don‚Äôt update running statistics.\n",
    "* This is important when generating text because we want **deterministic outputs**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ Disabling Gradient Tracking\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "```\n",
    "\n",
    "* Prevents PyTorch from computing gradients during generation.\n",
    "* **Why?**\n",
    "\n",
    "  * Generation is **inference**, not training.\n",
    "  * Saves memory and speeds up computation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ Generating Text from the Model\n",
    "\n",
    "```python\n",
    "output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "```\n",
    "\n",
    "* **`model.generate()`** produces **new tokens** one by one, starting from the given input.\n",
    "\n",
    "* Parameters:\n",
    "\n",
    "  | Parameter        | Description                                                               |\n",
    "  | ---------------- | ------------------------------------------------------------------------- |\n",
    "  | `input_tokens`   | The starting sequence (tensor) from which the model continues generating. |\n",
    "  | `max_new_tokens` | Maximum number of **new tokens** to generate. Here `100` tokens.          |\n",
    "\n",
    "* The output is a tensor of **token IDs** representing the generated text sequence, including both the original input and the new tokens.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6Ô∏è‚É£ Decoding the Generated Tokens\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(output[0].tolist()))\n",
    "```\n",
    "\n",
    "* **`output[0]`** ‚Üí Gets the generated sequence for the first (and only) batch.\n",
    "* **`.tolist()`** ‚Üí Converts the tensor into a Python list of token IDs.\n",
    "* **`tokenizer.decode()`** ‚Üí Converts token IDs back into **human-readable text**.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  [101, 1234, 5678, 3456, 7890] ‚Üí \"Salam labas! How are you today?\"\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ In simple words:\n",
    "\n",
    "1. Convert your starting text into token IDs the model understands.\n",
    "2. Move tokens to the device (CPU/GPU) and add batch dimension.\n",
    "3. Put the model in evaluation mode and disable gradients.\n",
    "4. Ask the model to generate a sequence of new tokens.\n",
    "5. Decode the generated tokens back into **readable text** and print it.\n",
    "\n",
    "---\n",
    "\n",
    "üìù **Note:**\n",
    "\n",
    "* You can adjust `max_new_tokens` to generate longer or shorter text.\n",
    "* For more creative or diverse outputs, you can also use parameters like `temperature`, `top_k`, or `top_p` in `generate()`.\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
