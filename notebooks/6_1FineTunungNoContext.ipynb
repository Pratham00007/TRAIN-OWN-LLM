{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325046f4",
   "metadata": {},
   "source": [
    "# Preapare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db4a23",
   "metadata": {},
   "source": [
    "### 1. Load the fine-tunung data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ed939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "file_path=\"../output/fine_tuning/data/fine_tuning.json\"\n",
    "with open (file_path,\"r\") as file:\n",
    "    data=json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b2e88",
   "metadata": {},
   "source": [
    "### 2. Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc095929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "tokenizer=RegexTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "\n",
    "def get_vocab_size(tokenizer: RegexTokenizer)-> int:\n",
    "    vocab=tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab)+len(special_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ec194",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üìå Purpose\n",
    "\n",
    "This script:\n",
    "\n",
    "1. Loads a trained tokenizer from a `.model` file (presumably trained on Darija).\n",
    "2. Defines a function to calculate the **total vocabulary size**, including:\n",
    "\n",
    "   * Regular tokens\n",
    "   * Special tokens (e.g. `<|pad|>`, `<|startoftext|>`, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Step-by-Step Breakdown\n",
    "\n",
    "### 1. ‚úÖ Import and Instantiate\n",
    "\n",
    "```python\n",
    "from minbpe import RegexTokenizer\n",
    "tokenizer = RegexTokenizer()\n",
    "```\n",
    "\n",
    "* `RegexTokenizer` is a customizable tokenizer from the `minbpe` library.\n",
    "* It uses regular expressions to tokenize text and can be trained or loaded from a saved model.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üì¶ Load Pretrained Tokenizer\n",
    "\n",
    "```python\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "```\n",
    "\n",
    "* Loads a previously trained tokenizer model from file.\n",
    "* This file contains token patterns, vocab, merges, and possibly special tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üßÆ Define Vocabulary Size Function\n",
    "\n",
    "```python\n",
    "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)\n",
    "```\n",
    "\n",
    "#### üî∏ `tokenizer.vocab`\n",
    "\n",
    "* A dictionary of learned tokens (subwords, character groups, etc.)\n",
    "\n",
    "#### üî∏ `tokenizer.special_tokens`\n",
    "\n",
    "* A dictionary or list of predefined tokens like:\n",
    "\n",
    "  * `<|pad|>`\n",
    "  * `<|startoftext|>`\n",
    "  * `<|endoftext|>`\n",
    "  * Custom ones like `<|seprator|>`\n",
    "\n",
    "#### ‚úÖ Return Value:\n",
    "\n",
    "* Total vocabulary size = number of normal tokens + number of special tokens\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example Output\n",
    "\n",
    "If the tokenizer contains:\n",
    "\n",
    "* 10,000 normal tokens\n",
    "* 5 special tokens\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "get_vocab_size(tokenizer)  # Returns 10005\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "This setup:\n",
    "\n",
    "* Loads a BPE tokenizer trained on a specific dataset\n",
    "* Calculates the full vocabulary size including special tokens\n",
    "* Useful when configuring models (e.g., `vocab_size` parameter in transformer models)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409dffe",
   "metadata": {},
   "source": [
    "### 3. Tokenize the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data=[]\n",
    "for item in data:\n",
    "    tokenized_item=tokenizer.encode(item,allowed_special=\"all\")\n",
    "    tokenized_data.append(tokenized_item)\n",
    "    \n",
    "len(tokenized_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be746d9",
   "metadata": {},
   "source": [
    "\n",
    "## üìå Purpose\n",
    "\n",
    "This script takes a list of text entries (likely training samples) and **tokenizes** them using your previously loaded `RegexTokenizer`. It then checks how many tokens are in the **first item** of the tokenized dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Step-by-Step Breakdown\n",
    "\n",
    "### 1. üìã Initialize List\n",
    "\n",
    "```python\n",
    "tokenized_data = []\n",
    "```\n",
    "\n",
    "* A list to store tokenized versions of each string from `data`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üîÑ Loop Through `data`\n",
    "\n",
    "```python\n",
    "for item in data:\n",
    "```\n",
    "\n",
    "* `data` is assumed to be a list of strings ‚Äî probably the same kind of entries as in `fine_tuning_data`, like:\n",
    "\n",
    "  ```\n",
    "  <|startoftext|>Alice<|seprator|>Hello\\nHow are you?<|endoftext|>\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üß© Tokenize Each String\n",
    "\n",
    "```python\n",
    "tokenized_item = tokenizer.encode(item, allowed_special=\"all\")\n",
    "```\n",
    "\n",
    "* `tokenizer.encode()` breaks the string into tokens using rules from your BPE model.\n",
    "* `allowed_special=\"all\"` tells the tokenizer to **recognize and preserve special tokens**, rather than splitting or ignoring them.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚ûï Append Tokenized Output\n",
    "\n",
    "```python\n",
    "tokenized_data.append(tokenized_item)\n",
    "```\n",
    "\n",
    "* Adds the tokenized version of each string to the `tokenized_data` list.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. üìè Check Token Count of First Item\n",
    "\n",
    "```python\n",
    "len(tokenized_data[0])\n",
    "```\n",
    "\n",
    "* Returns the **number of tokens** in the first tokenized string (i.e., how long the first training sample is after tokenization).\n",
    "* This is useful for:\n",
    "\n",
    "  * Debugging token lengths\n",
    "  * Preparing inputs for models that have max token limits (e.g., 512 for many transformers)\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example\n",
    "\n",
    "If your first string is:\n",
    "\n",
    "```text\n",
    "<|startoftext|>Alice<|seprator|>Hello<|endoftext|>\n",
    "```\n",
    "\n",
    "The tokenizer might return something like:\n",
    "\n",
    "```python\n",
    "[1, 482, 3, 1025, 2]  # (Token IDs)\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "len(tokenized_data[0])  # Returns 5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* This code tokenizes a list of strings using a pretrained tokenizer.\n",
    "* Special tokens are preserved.\n",
    "* The length of the first tokenized item is measured ‚Äî useful for understanding sequence sizes in training data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4e801",
   "metadata": {},
   "source": [
    "### 4. Spliting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3890d8e5",
   "metadata": {},
   "source": [
    "We need to keep the multi-turn conversations complete in each part\n",
    "\n",
    "Training and Testing sets start with ```You``` message and end with an ```Assistant``` message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inititial_split_index=int(0.95 * len(data))\n",
    "\n",
    "#adjusting the index to ensure that the trainingset ends with Assistant message\n",
    "# and validation set start with \"You\" message\n",
    "\n",
    "# scanning backward to find an assistant message\n",
    "split_index=inititial_split_index\n",
    "while split_index>0 and not data[split_index-1].startswith('<|startoftext|>Assistant'):\n",
    "    split_index -=1\n",
    "\n",
    "train_data = data[:split_index]\n",
    "val_data=data[split_index:]\n",
    "\n",
    "print(\"Training set: \")\n",
    "print(f\"Start message: {train_data[0].split('<|separator|>')[0]}\")\n",
    "print(f\"End message: {train_data[-1].split('<|separator|>')[0]}\")\n",
    "\n",
    "print(\"\\n Validation Set\")\n",
    "print(f\"Start message: {val_data[0].split('<|separator|>')[0]}\")\n",
    "print(f\"End message: {val_data[-1].split('<|separator|>')[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa776f",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Explanation: Smart Dataset Splitting to Preserve Dialogue Integrity\n",
    "\n",
    "This logic is designed to split a dataset of formatted conversation messages (stored in `data`) into training and validation sets **without breaking the natural flow of dialogue** ‚Äî particularly ensuring the training set ends with an Assistant response and the validation set begins with a User message.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Step 1: Define Initial Split Point\n",
    "\n",
    "```python\n",
    "inititial_split_index = int(0.95 * len(data))\n",
    "```\n",
    "\n",
    "* Calculates the initial split index at 95% of the dataset.\n",
    "* This is a **common ratio** for fine-tuning: 95% training, 5% validation.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 2: Backtrack to the Last Assistant Message\n",
    "\n",
    "```python\n",
    "split_index = inititial_split_index\n",
    "\n",
    "while split_index > 0 and not data[split_index - 1].startswith('<|startoftext|>Assistant'):\n",
    "    split_index -= 1\n",
    "```\n",
    "\n",
    "* The code **backtracks from the 95% mark** to ensure the last training message is from the Assistant.\n",
    "\n",
    "* This is important because many models learn from alternating patterns ‚Äî e.g., User message ‚Üí Assistant response.\n",
    "\n",
    "* If training data ends in the middle of a user input, the model may struggle to learn proper turn-taking.\n",
    "\n",
    "* The loop checks each message (going backward) to find the first one that starts with:\n",
    "\n",
    "  ```\n",
    "  <|startoftext|>Assistant\n",
    "  ```\n",
    "\n",
    "* Once found, that index becomes the new split point.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 3: Slice the Dataset\n",
    "\n",
    "```python\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]\n",
    "```\n",
    "\n",
    "* `train_data` contains all messages **up to and including** the last complete Assistant response.\n",
    "* `val_data` contains the remaining messages, **starting from the next user prompt**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 4: Print Metadata About the Split\n",
    "\n",
    "```python\n",
    "print(\"Training set: \")\n",
    "print(f\"Start message: {train_data[0].split('<|separator|>')[0]}\")\n",
    "print(f\"End message: {train_data[-1].split('<|separator|>')[0]}\")\n",
    "\n",
    "print(\"\\n Validation Set\")\n",
    "print(f\"Start message: {val_data[0].split('<|separator|>')[0]}\")\n",
    "print(f\"End message: {val_data[-1].split('<|separator|>')[0]}\")\n",
    "```\n",
    "\n",
    "* Each message is formatted like:\n",
    "\n",
    "  ```\n",
    "  <|startoftext|>Assistant<|separator|>message content<|endoftext|>\n",
    "  ```\n",
    "* `split('<|separator|>')[0]` extracts the **sender** part of the message.\n",
    "* This printout confirms:\n",
    "\n",
    "  * The training set starts and ends with the right roles.\n",
    "  * The validation set starts correctly with a user message, following a full Assistant response in the training set.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why This Matters\n",
    "\n",
    "* **Maintains conversational context**: Models trained on structured conversations benefit when training and validation sets reflect complete message pairs.\n",
    "* **Avoids broken samples**: Prevents the training set from ending mid-dialogue, which could degrade model quality.\n",
    "* **Ensures natural flow**: Validation accuracy is more meaningful when the validation set starts with a user message and follows a realistic conversation thread.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2098d8",
   "metadata": {},
   "source": [
    "spliting tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6248a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenized_data[:split_index]\n",
    "val_data = tokenized_data[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a98c9",
   "metadata": {},
   "source": [
    "combine `you` and `Assistant` turns into one sequence. but make sure resulting sequence  does not exceed the `block_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbd196",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=256\n",
    "\n",
    "def combined_turns(data: list[list[int]],should_trim_long_sequence: bool) -> list[list[int]]:\n",
    "    combined_turns_data = []\n",
    "    for i in range(0,len(data)-1,2):\n",
    "        you_message=data[i]\n",
    "        assistant_message=data[i+1]\n",
    "        if not you_message or not assistant_message:\n",
    "            continue \n",
    "\n",
    "        final_message=you_message+assistant_message\n",
    "        if len(final_message)>block_size and should_trim_long_sequence:\n",
    "            final_message=final_message[-block_size:]\n",
    "\n",
    "        combined_turns_data.append(final_message)\n",
    "    return combined_turns_data\n",
    "combined_val_data=combined_turns(\n",
    "    data=train_data,\n",
    "    should_trim_long_sequence=True\n",
    ")\n",
    "\n",
    "combined_val_data=combined_turns(\n",
    "    data=val_dal,\n",
    "    should_trim_long_sequence=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7b851",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Combining Paired Turns into Single Token Sequences for Model Training\n",
    "\n",
    "This logic is designed to **combine user and assistant messages** into a single token sequence that fits within a defined `block_size`. This is a critical preprocessing step for training transformer models that expect inputs as flat sequences of tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Concepts\n",
    "\n",
    "* **Paired Turns**: User and Assistant messages are expected to alternate. Each \"turn\" consists of one user message followed by one assistant reply.\n",
    "* **Block Size**: The maximum number of tokens that a sequence should contain (set here to `256`).\n",
    "* **Trimming**: Sequences longer than the block size can be trimmed from the beginning, preserving the most recent tokens (typically more relevant in conversations).\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Step-by-Step Breakdown\n",
    "\n",
    "### üîπ `block_size = 256`\n",
    "\n",
    "Defines the maximum length for a single input sequence (in tokens). This is often constrained by model architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Function Definition: `combined_turns(...)`\n",
    "\n",
    "```python\n",
    "def combined_turns(data: list[list[int]], should_trim_long_sequence: bool) -> list[list[int]]:\n",
    "```\n",
    "\n",
    "* **Input**:\n",
    "\n",
    "  * `data`: A list of tokenized messages, where each message is a list of integers (token IDs).\n",
    "  * `should_trim_long_sequence`: A boolean flag indicating whether to trim sequences that exceed the block size.\n",
    "\n",
    "* **Output**:\n",
    "\n",
    "  * Returns a list of **combined input sequences**, where each item includes both user and assistant messages in a single list of tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Loop Through Paired Messages\n",
    "\n",
    "```python\n",
    "for i in range(0, len(data)-1, 2):\n",
    "```\n",
    "\n",
    "* Iterates through the data **two messages at a time** (step of 2).\n",
    "* This assumes the data is **ordered as User, Assistant, User, Assistant, ...**\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Assign Messages\n",
    "\n",
    "```python\n",
    "you_message = data[i]\n",
    "assistant_message = data[i+1]\n",
    "```\n",
    "\n",
    "* Picks the current user message and the immediate next assistant message.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Skip Incomplete Pairs\n",
    "\n",
    "```python\n",
    "if not you_message or not assistant_message:\n",
    "    continue\n",
    "```\n",
    "\n",
    "* If either message is missing (e.g., empty or `None`), skip this pair.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Combine and Trim If Needed\n",
    "\n",
    "```python\n",
    "final_message = you_message + assistant_message\n",
    "if len(final_message) > block_size and should_trim_long_sequence:\n",
    "    final_message = final_message[-block_size:]\n",
    "```\n",
    "\n",
    "* Combines the two messages into a single sequence.\n",
    "* If the result exceeds the `block_size` and trimming is allowed:\n",
    "\n",
    "  * Trims from the **beginning**, keeping only the last `block_size` tokens.\n",
    "  * This favors more recent dialogue, which is often more contextually relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Append to Output\n",
    "\n",
    "```python\n",
    "combined_turns_data.append(final_message)\n",
    "```\n",
    "\n",
    "* Adds the finalized token sequence to the output list.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Apply to Training and Validation Sets\n",
    "\n",
    "```python\n",
    "combined_val_data = combined_turns(data=train_data, should_trim_long_sequence=True)\n",
    "combined_val_data = combined_turns(data=val_dal, should_trim_long_sequence=True)\n",
    "```\n",
    "\n",
    "* **First call** processes the `train_data` token sequences.\n",
    "* **Second call** attempts to process `val_dal`, which appears to be a typo ‚Äî it should likely be `val_data`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùó Important Note\n",
    "\n",
    "```python\n",
    "combined_val_data = combined_turns(data=val_dal, should_trim_long_sequence=True)\n",
    "```\n",
    "\n",
    "* **Typo Alert**: `val_dal` should be corrected to `val_data` to avoid a `NameError`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* This process merges alternating User and Assistant messages into training samples of token sequences.\n",
    "* Ensures that each input sample fits within a maximum length (`block_size`).\n",
    "* Preserves recent context by trimming from the start if needed.\n",
    "* Prepares data in a format compatible with autoregressive transformer models like GPT.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data\")\n",
    "print(f\"Length before: {len(train_data)}\")\n",
    "print(f\"Length after: {len(combined_train_data)}\")\n",
    "\n",
    "print(\"\\nValidation data\")\n",
    "print(f\"Length before: {len(val_data)}\")\n",
    "print(f\"Length after: {len(combined_val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b25bea",
   "metadata": {},
   "source": [
    "convert each sequence of tokens into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d574ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_data=torch.tensor(combined_train_data)\n",
    "val_data=torch.tensor(combined_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72deb9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Converting Tokenized Data into PyTorch Tensors\n",
    "\n",
    "This snippet converts your preprocessed token sequences into **PyTorch tensors**, which are the primary data structure used for model training in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Convert Training Data\n",
    "\n",
    "```python\n",
    "train_data = torch.tensor(combined_train_data)\n",
    "```\n",
    "\n",
    "* Takes `combined_train_data` (a list of lists of integers representing token IDs).\n",
    "* Converts it into a **PyTorch tensor** with shape `[num_samples, sequence_length]`.\n",
    "* Enables fast, GPU-accelerated operations during model training.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Convert Validation Data\n",
    "\n",
    "```python\n",
    "val_data = torch.tensor(combined_val_data)\n",
    "```\n",
    "\n",
    "* Similarly converts validation sequences to a tensor.\n",
    "* Allows efficient evaluation of the model during training.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Use PyTorch Tensors?\n",
    "\n",
    "* Tensors are optimized for mathematical operations, including backpropagation.\n",
    "* They seamlessly integrate with PyTorch‚Äôs DataLoaders and neural network modules.\n",
    "* Support GPU acceleration for faster training.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Assumptions and Tips\n",
    "\n",
    "* The sequences in `combined_train_data` and `combined_val_data` are expected to be **uniform in length** (likely due to trimming or padding).\n",
    "* If sequences vary in length, consider padding them before conversion, or use PyTorch‚Äôs `PackedSequence` utilities.\n",
    "* Using tensors is a necessary step before feeding data into most PyTorch models.\n",
    "\n",
    "---\n",
    "\n",
    "This completes the pipeline from raw message data to tensor-ready inputs for a PyTorch model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c090b1",
   "metadata": {},
   "source": [
    "token sequence is not of same length so cant convert turn into tensor all at once\n",
    "so for taht convert it into same length\n",
    "\n",
    "use padding to fix the prb. add in start and at end of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acdf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)\n",
    "\n",
    "#the token <|padding|> is used to mask the padding tokens.\n",
    "# masking mean the model will ignore these tokens during trainning\n",
    "# ie loss will not be calculated for those\n",
    "\n",
    "pading_tokens=tokenizer.special_tokens[\"|<padding|>\"]\n",
    "\n",
    "def apply_padding_to_data(data: list[list[int]],block_size: int,padding_token: int)-> torch.Tensor :\n",
    "    tensors=[]\n",
    "    for i in range(len(data)):\n",
    "        tensor = torch.tensor(data[i])\n",
    "        padded_tensor=tensor.nn.functional.pad(\n",
    "            input=tensor,\n",
    "            # for rigth padding\n",
    "            pad=(0,block_size-len(tensor)),\n",
    "            #pad=(0,block_size-len(tensor),0),\n",
    "            value=padding_token\n",
    "        )\n",
    "        tensors.append(padded_tensor)\n",
    "\n",
    "    return torch.stack(tensors)\n",
    "\n",
    "train_data_tensor = apply_padding_to_data(\n",
    "    data=combined_train_data,\n",
    "    block_size=block_size,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "\n",
    "val_data_tensor = apply_padding_to_data(\n",
    "    data=combined_val_data,\n",
    "    block_size=block_size,\n",
    "    padding_token=padding_token \n",
    ")\n",
    "\n",
    "val_data=tensor=apply_padding_to_data(\n",
    "    data=combined_val_data,\n",
    "    block_size=block_size,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "\n",
    "train_data_tensor.shape,val_data_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba8bbb2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Padding Token Sequences for Uniform Length Using PyTorch\n",
    "\n",
    "This code snippet prepares your tokenized sequences for model training by **right-padding** them to a fixed `block_size`. Padding ensures that all sequences have the same length, which is necessary for batch processing in neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Set Random Seed\n",
    "\n",
    "```python\n",
    "torch.manual_seed(3647)\n",
    "```\n",
    "\n",
    "* Fixes randomness for reproducibility (important in experiments).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Identify Padding Token ID\n",
    "\n",
    "```python\n",
    "padding_token = tokenizer.special_tokens[\"|<padding|>\"]\n",
    "```\n",
    "\n",
    "* Retrieves the integer ID for the special padding token.\n",
    "* This token will be used to fill sequences that are shorter than `block_size`.\n",
    "\n",
    "> **Note:** There seems to be a typo in the key: `\"|<padding|>\"` ‚Äî typically it should be `\"<|padding|>\"`. Make sure this matches your tokenizer‚Äôs actual special token key.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Define Padding Function\n",
    "\n",
    "```python\n",
    "def apply_padding_to_data(data: list[list[int]], block_size: int, padding_token: int) -> torch.Tensor:\n",
    "```\n",
    "\n",
    "* **Inputs**:\n",
    "\n",
    "  * `data`: List of token sequences (lists of ints).\n",
    "  * `block_size`: Desired fixed sequence length.\n",
    "  * `padding_token`: Token ID to use for padding.\n",
    "\n",
    "* **Process**:\n",
    "\n",
    "  * Converts each sequence to a PyTorch tensor.\n",
    "  * Pads sequences on the **right side** to reach `block_size` tokens.\n",
    "  * Uses `torch.nn.functional.pad` with parameters:\n",
    "\n",
    "    * `pad=(0, block_size - len(tensor))`: pads zeros on the right.\n",
    "    * `value=padding_token`: fills padding with the padding token ID.\n",
    "  * Collects all padded tensors.\n",
    "\n",
    "* **Returns**:\n",
    "\n",
    "  * A stacked tensor of shape `[num_sequences, block_size]`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Pad Training and Validation Data\n",
    "\n",
    "```python\n",
    "train_data_tensor = apply_padding_to_data(\n",
    "    data=combined_train_data,\n",
    "    block_size=block_size,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "\n",
    "val_data_tensor = apply_padding_to_data(\n",
    "    data=combined_val_data,\n",
    "    block_size=block_size,\n",
    "    padding_token=padding_token \n",
    ")\n",
    "```\n",
    "\n",
    "* Applies the padding function to both training and validation datasets.\n",
    "* Converts variable-length sequences into uniform-length tensors suitable for batching.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Duplicate Line to Pad Validation Data Again\n",
    "\n",
    "```python\n",
    "val_data = tensor = apply_padding_to_data(\n",
    "    data=combined_val_data,\n",
    "    block_size=block_size,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "```\n",
    "\n",
    "* This line appears to redundantly pad `combined_val_data` again and assign it to `val_data` and `tensor`.\n",
    "* This can be simplified or removed unless there's a reason to have a separate variable.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Final Shape Output\n",
    "\n",
    "```python\n",
    "train_data_tensor.shape, val_data_tensor.shape\n",
    "```\n",
    "\n",
    "* Returns the shape of the padded tensors.\n",
    "* Should output something like `(num_train_samples, block_size)` and `(num_val_samples, block_size)` confirming successful padding.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* Padding is essential to ensure fixed-length inputs.\n",
    "* Padding tokens are masked during training, so they don‚Äôt affect loss computation.\n",
    "* Right-padding preserves the original token order and sequence start.\n",
    "* The output tensors are ready for batch feeding into a transformer model.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ö†Ô∏è Notes and Potential Fixes\n",
    "\n",
    "* Correct the padding token key if necessary (`\"<|padding|>\"` vs `\"|<padding|>\"`).\n",
    "* `tensor.nn.functional.pad` should be `torch.nn.functional.pad` (padding is a function in `torch.nn.functional` module, not a tensor method).\n",
    "* The redundant assignment of `val_data` could be removed to avoid confusion.\n",
    "\n",
    "---\n",
    "\n",
    "This ensures your data is properly padded and formatted for model training with PyTorch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f846b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb32557",
   "metadata": {},
   "source": [
    "### 5. Create the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbbb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e845de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(self,data: torch.Tensor,device:torch.device , padding_token: int):\n",
    "        self.data=data #shape: (num_samples, block_Size)\n",
    "        self.device=device\n",
    "        self.padding_token=padding_token\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index: int)->Tuple[torch.Tensor,torch.Tensor]:\n",
    "        sample=self.data[index]\n",
    "        x=sample.to(self.device)\n",
    "        y=sample[1:].to(self.device)\n",
    "        padding_tensor=torch.tensor([self.padding_token],device=self.device)\n",
    "        y=torch.cat((y,padding_tensor))\n",
    "        return x,y\n",
    "    \n",
    "batch_size=64\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset=FineTuningDataset(\n",
    "    data=train_data_tensor,\n",
    "    device=device,\n",
    "    padding_token=padding_token\n",
    "\n",
    ")\n",
    "train_loader=DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset=FineTuningDataset(\n",
    "    data=val_data_tensor,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "val_loader=DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168afb3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Creating a Custom Dataset and DataLoader for Fine-Tuning with PyTorch\n",
    "\n",
    "This code defines a **custom PyTorch Dataset class** for handling tokenized conversation data and prepares `DataLoader`s for efficient batch training and validation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ `FineTuningDataset` Class\n",
    "\n",
    "* Inherits from `torch.utils.data.Dataset`, enabling seamless integration with PyTorch‚Äôs data pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ `__init__` Method\n",
    "\n",
    "```python\n",
    "def __init__(self, data: torch.Tensor, device: torch.device, padding_token: int):\n",
    "    self.data = data  # shape: (num_samples, block_size)\n",
    "    self.device = device\n",
    "    self.padding_token = padding_token\n",
    "```\n",
    "\n",
    "* Stores the tokenized and padded dataset as a tensor.\n",
    "* Stores the device (CPU or GPU) for efficient data transfer.\n",
    "* Keeps track of the padding token ID, used later for target sequence alignment.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ `__len__` Method\n",
    "\n",
    "```python\n",
    "def __len__(self) -> int:\n",
    "    return len(self.data)\n",
    "```\n",
    "\n",
    "* Returns the number of samples in the dataset.\n",
    "* Enables use of `len()` on the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ `__getitem__` Method\n",
    "\n",
    "```python\n",
    "def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    sample = self.data[index]\n",
    "    x = sample.to(self.device)\n",
    "    y = sample[1:].to(self.device)\n",
    "    padding_tensor = torch.tensor([self.padding_token], device=self.device)\n",
    "    y = torch.cat((y, padding_tensor))\n",
    "    return x, y\n",
    "```\n",
    "\n",
    "* Retrieves one token sequence sample by index.\n",
    "* **Inputs** (`x`):\n",
    "\n",
    "  * The full token sequence.\n",
    "* **Targets** (`y`):\n",
    "\n",
    "  * The token sequence shifted one step **to the left**, i.e., from token 1 to end.\n",
    "  * Padding token appended at the end to keep `y` the same length as `x`.\n",
    "* This setup prepares sequences for **autoregressive training** where the model predicts the next token.\n",
    "* Moves tensors to the appropriate device.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Dataset and DataLoader Instantiation\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Batch Size and Device\n",
    "\n",
    "```python\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    "* Sets a batch size of 64 samples per batch.\n",
    "* Chooses GPU if available; otherwise, CPU.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Create Train Dataset and Loader\n",
    "\n",
    "```python\n",
    "train_dataset = FineTuningDataset(\n",
    "    data=train_data_tensor,\n",
    "    device=device,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "```\n",
    "\n",
    "* Wraps training data tensor in the custom dataset.\n",
    "* Creates a DataLoader to provide shuffled batches during training.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Create Validation Dataset and Loader\n",
    "\n",
    "```python\n",
    "val_dataset = FineTuningDataset(\n",
    "    data=val_data_tensor,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "```\n",
    "\n",
    "* Wraps validation data similarly.\n",
    "* Validation loader does **not shuffle** to keep evaluation deterministic.\n",
    "\n",
    "> **Note:** `val_dataset` does not specify `device`. This means it won‚Äôt move validation data to GPU automatically. You might want to either add `device=device` or move batches manually during evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* This setup allows easy batching and feeding of tokenized sequences to a transformer model.\n",
    "* The target sequences are shifted versions of inputs for predicting the next token.\n",
    "* Using PyTorch `DataLoader` provides efficient iteration, batching, and shuffling.\n",
    "* Device handling within the dataset helps with minimizing manual tensor transfers during training.\n",
    "\n",
    "---\n",
    "\n",
    "This class and loaders form the backbone for training and validating autoregressive language models on conversation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef809ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad027cc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Retrieving a Batch from the DataLoader\n",
    "\n",
    "This line of code:\n",
    "\n",
    "```python\n",
    "x, y = next(iter(train_loader))\n",
    "```\n",
    "\n",
    "performs a **single batch extraction** from the training data loader.\n",
    "\n",
    "---\n",
    "\n",
    "## What Happens Here?\n",
    "\n",
    "* `iter(train_loader)` creates an **iterator** over the `train_loader`.\n",
    "* `next(...)` fetches the **first batch** from that iterator.\n",
    "* The batch is unpacked into:\n",
    "\n",
    "  * `x`: a tensor containing the input sequences (shape: `[batch_size, block_size]`).\n",
    "  * `y`: a tensor containing the target sequences (shifted inputs for next-token prediction).\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use This?\n",
    "\n",
    "* Useful for **quick inspection** or debugging to check shapes, data types, and content of your batches.\n",
    "* Ensures your data pipeline is working as expected before starting training.\n",
    "* Can be used to test model input/output compatibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Typical Use Case\n",
    "\n",
    "```python\n",
    "print(x.shape)  # Expected: (batch_size, block_size)\n",
    "print(y.shape)  # Expected: (batch_size, block_size)\n",
    "```\n",
    "\n",
    "This confirms batch dimensions and helps verify padding and shifting are correct.\n",
    "\n",
    "---\n",
    "\n",
    "This step is a straightforward way to peek into your training batches before feeding them into the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b60c23",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb03a4",
   "metadata": {},
   "source": [
    "### 1. Load the saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size=256\n",
    "n_embd=512\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    "    ignore_index=tokenizer.special_tokens[\"<|padding|>\"],\n",
    "\n",
    ").to(device)\n",
    "model=torch.compile(model)\n",
    "print(sum(p.numel() for p in model.parameter())/1e6, 'M parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127732d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Initializing and Compiling a GPT Language Model for Fine-Tuning\n",
    "\n",
    "This snippet sets up a GPT-based language model with specified hyperparameters, prepares it for training on your dataset, and prints out the total number of model parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Define Model Hyperparameters\n",
    "\n",
    "```python\n",
    "block_size = 256      # Maximum context length (sequence length)\n",
    "n_embd = 512          # Embedding dimension size\n",
    "n_head = 8            # Number of attention heads in each Transformer block\n",
    "n_layer = 4           # Number of Transformer layers (depth)\n",
    "dropout = 0.2         # Dropout rate for regularization\n",
    "batch_size = 64       # Batch size for training\n",
    "vocab_size = get_vocab_size(tokenizer)  # Vocabulary size based on tokenizer\n",
    "```\n",
    "\n",
    "* These hyperparameters configure the model's architecture and capacity.\n",
    "* `block_size` matches the input sequence length you prepared.\n",
    "* `vocab_size` corresponds to the tokenizer's vocabulary size plus special tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Instantiate the Model\n",
    "\n",
    "```python\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    "    ignore_index=tokenizer.special_tokens[\"<|padding|>\"],\n",
    ").to(device)\n",
    "```\n",
    "\n",
    "* Creates a GPT model with the specified architecture.\n",
    "* Passes the device (CPU or GPU) for proper allocation.\n",
    "* Sets `ignore_index` to the padding token so the loss function ignores padded tokens during training.\n",
    "* The model is then moved to the selected device using `.to(device)`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Compile the Model\n",
    "\n",
    "```python\n",
    "model = torch.compile(model)\n",
    "```\n",
    "\n",
    "* Uses PyTorch 2.0's `torch.compile()` feature to optimize the model's execution.\n",
    "* Compilation can speed up training and inference by leveraging backend optimizations.\n",
    "* Note: Requires PyTorch 2.0+ and compatible hardware.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Print Model Size\n",
    "\n",
    "```python\n",
    "print(sum(p.numel() for p in model.parameter()) / 1e6, 'M parameter')\n",
    "```\n",
    "\n",
    "* Calculates the total number of parameters in millions.\n",
    "* Useful for understanding model complexity and estimating training resources.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* This sets up a moderately sized GPT model tailored to your tokenizer and dataset.\n",
    "* Incorporates device allocation, padding token handling, and runtime optimizations.\n",
    "* The printed parameter count gives insight into model scale before training begins.\n",
    "\n",
    "---\n",
    "\n",
    "This prepares the core model architecture ready for fine-tuning on your dialogue data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../output/pre_training/base/epoch_5.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f0b77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Loading a Pre-Trained Checkpoint into the Model\n",
    "\n",
    "This snippet demonstrates how to **load pre-trained weights** from a saved checkpoint into your GPT model before fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Define Checkpoint Path\n",
    "\n",
    "```python\n",
    "checkpoint_path = \"../output/pre_training/base/epoch_5.pth\"\n",
    "```\n",
    "\n",
    "* Specifies the file path where the pre-trained model checkpoint is saved.\n",
    "* Usually, this checkpoint contains the weights after training for 5 epochs (as suggested by the filename).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Load Checkpoint File\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "```\n",
    "\n",
    "* Loads the checkpoint file from disk into memory.\n",
    "* The argument `weights_only=True` hints to load only model weights, ignoring optimizer states or other metadata (may depend on your PyTorch version).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Extract Model State Dictionary\n",
    "\n",
    "```python\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "```\n",
    "\n",
    "* Retrieves the saved dictionary of model parameters (`state_dict`) from the checkpoint.\n",
    "* `state_dict` maps parameter names to their tensor values.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Load Weights into Model\n",
    "\n",
    "```python\n",
    "model.load_state_dict(model_state_dict)\n",
    "```\n",
    "\n",
    "* Updates the model‚Äôs parameters with the pre-trained weights.\n",
    "* Ensures that the model starts from a trained initialization rather than random weights.\n",
    "* Essential for transfer learning or continued training.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* Loading a pre-trained checkpoint accelerates convergence and can improve performance.\n",
    "* This step prepares your GPT model to fine-tune on your specific data with previously learned knowledge.\n",
    "* Make sure the model architecture matches the checkpoint to avoid errors when loading weights.\n",
    "\n",
    "---\n",
    "\n",
    "This sets your model up with pre-trained knowledge before beginning fine-tuning on your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a127fd",
   "metadata": {},
   "source": [
    "Generate from the model to make sure that the weights were loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79216297",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(\"Salam labas \", allowed_special=\"all\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c37ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Generating Text with the Fine-Tuned GPT Model\n",
    "\n",
    "This code snippet shows how to **generate text** (e.g., a chatbot reply) from your trained GPT model given an input prompt.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Tokenize Input Prompt\n",
    "\n",
    "```python\n",
    "input_tokens = tokenizer.encode(\"Salam labas \", allowed_special=\"all\")\n",
    "input_tokens = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "```\n",
    "\n",
    "* Encodes the string `\"Salam labas \"` into token IDs using your tokenizer.\n",
    "* `allowed_special=\"all\"` allows special tokens in the input if needed.\n",
    "* Converts the token list into a PyTorch tensor with shape `[1, sequence_length]` (`unsqueeze(0)` adds batch dimension).\n",
    "* Moves the tensor to the appropriate device (CPU or GPU).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Set Model to Evaluation Mode\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "* Switches the model to evaluation mode.\n",
    "* Disables dropout and other training-specific behaviors for consistent output.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Generate Text Without Gradient Tracking\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "```\n",
    "\n",
    "* Disables gradient computation to save memory and speed up inference.\n",
    "* Calls the model‚Äôs `.generate()` method to produce up to 100 new tokens following the input.\n",
    "* The generation method uses the model‚Äôs autoregressive property to predict next tokens one-by-one.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Decode and Print Output\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(output[0].tolist()))\n",
    "```\n",
    "\n",
    "* Converts the generated token IDs back into a human-readable string.\n",
    "* Prints the generated text continuation, including the input prompt plus generated tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* This is a standard approach for interactive text generation with transformer language models.\n",
    "* The input prompt seeds the generation, and the model produces a relevant continuation.\n",
    "* Useful for chatbots, story generation, or any natural language generation task.\n",
    "\n",
    "---\n",
    "\n",
    "You get a quick demonstration of your fine-tuned GPT‚Äôs ability to understand and continue conversations in the target language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb479e9",
   "metadata": {},
   "source": [
    "### 2. Estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    ") -> Dict[str, float]:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = []\n",
    "        for x, y in loader:\n",
    "            with torch.no_grad():\n",
    "                _, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "        output[split] = sum(losses) / len(losses)\n",
    "\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defcbaf6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Function to Estimate Average Training and Validation Loss\n",
    "\n",
    "This function computes the **average loss** of the model over the entire training and validation datasets, without updating the model weights. It helps monitor performance and overfitting during training.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Function Signature and Decorator\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    ") -> Dict[str, float]:\n",
    "```\n",
    "\n",
    "* Decorated with `@torch.no_grad()` to disable gradient calculation throughout the function, reducing memory usage and speeding up evaluation.\n",
    "* Accepts:\n",
    "\n",
    "  * `model`: The PyTorch model to evaluate.\n",
    "  * `train_loader` and `val_loader`: DataLoaders for training and validation sets.\n",
    "* Returns a dictionary with average losses, e.g., `{\"train\": 0.12, \"val\": 0.15}`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Set Model to Evaluation Mode\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "* Switches the model to evaluation mode to deactivate training behaviors like dropout.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Compute Loss for Each Split\n",
    "\n",
    "```python\n",
    "for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "    losses = []\n",
    "    for x, y in loader:\n",
    "        with torch.no_grad():\n",
    "            _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    output[split] = sum(losses) / len(losses)\n",
    "```\n",
    "\n",
    "* Loops over the training and validation datasets.\n",
    "* For each batch `(x, y)`:\n",
    "\n",
    "  * Calls the model to get the loss (assuming the model returns a tuple with loss as the second element).\n",
    "  * Collects the scalar loss values.\n",
    "* Calculates the **mean loss** over all batches per split.\n",
    "* Stores results in the output dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Restore Model to Training Mode\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "```\n",
    "\n",
    "* Switches the model back to training mode after evaluation to resume normal training behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Return Average Losses\n",
    "\n",
    "```python\n",
    "return output\n",
    "```\n",
    "\n",
    "* Returns a dictionary with average training and validation losses, useful for logging and early stopping.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* Provides a clean way to evaluate the model‚Äôs performance on both datasets without affecting gradients.\n",
    "* Helps track progress during training and identify overfitting or underfitting.\n",
    "* Efficiently handles large datasets by using DataLoaders and disabling gradients.\n",
    "\n",
    "---\n",
    "\n",
    "This function is a key utility to monitor model learning quality during fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278407c7",
   "metadata": {},
   "source": [
    "### Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea374ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c17f2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Saving a Training Checkpoint\n",
    "\n",
    "This function saves the current state of the training process into a file, enabling you to **pause and resume training** later without losing progress.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Function Signature\n",
    "\n",
    "```python\n",
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "```\n",
    "\n",
    "* Accepts:\n",
    "\n",
    "  * `model`: The GPT model being trained.\n",
    "  * `optimizer`: The optimizer used during training.\n",
    "  * `epoch`: The current epoch number.\n",
    "  * `loss`: The latest loss value (e.g., validation loss).\n",
    "  * `file_path`: Where to save the checkpoint file (default: `\"checkpoint.pth\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Prepare Checkpoint Dictionary\n",
    "\n",
    "```python\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "}\n",
    "```\n",
    "\n",
    "* Stores:\n",
    "\n",
    "  * The current epoch for resuming training.\n",
    "  * The model‚Äôs parameters (`state_dict`) to restore weights.\n",
    "  * The optimizer‚Äôs parameters (`state_dict`) to preserve optimizer state (learning rate, momentum, etc.).\n",
    "  * The loss value for monitoring or bookkeeping.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Save to Disk\n",
    "\n",
    "```python\n",
    "torch.save(checkpoint, file_path)\n",
    "```\n",
    "\n",
    "* Serializes and writes the checkpoint dictionary to the specified file path.\n",
    "* Allows future loading via `torch.load()` for resuming or evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* Checkpointing is essential for:\n",
    "\n",
    "  * Recovering from interruptions (e.g., crashes, power failures).\n",
    "  * Performing model evaluation at certain training milestones.\n",
    "  * Experimenting with different training regimes without losing progress.\n",
    "* This function captures all critical training state components to enable seamless resumption.\n",
    "\n",
    "---\n",
    "\n",
    "This function provides a reliable way to persist training progress during your fine-tuning workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cc7a31",
   "metadata": {},
   "source": [
    "### 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 20\n",
    "eval_interval = 20\n",
    "learning_rate = 6e-5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Evaluation\n",
    "        if batch_idx % eval_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "            losses = estimate_loss(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "            )\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "\n",
    "            print(\n",
    "                f\"iteration {iteration} / step {batch_idx}: \"\n",
    "                f\"train loss {losses['train']:.4f}, \"\n",
    "                f\"val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Training step\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=iteration,\n",
    "        loss=loss.item(),\n",
    "        file_path=f\"../output/fine_tuning/run_3/checkpoint_{iteration}.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666db913",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Fine-Tuning Loop with Periodic Evaluation and Checkpointing\n",
    "\n",
    "This code snippet runs the **training loop** for your GPT model with regular evaluation on both training and validation datasets, and saves checkpoints at the end of each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Define Training Parameters and Optimizer\n",
    "\n",
    "```python\n",
    "max_iters = 20         # Number of training epochs\n",
    "eval_interval = 20     # Frequency (in batches) to evaluate model loss\n",
    "learning_rate = 6e-5   # Learning rate for AdamW optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "```\n",
    "\n",
    "* Sets how long and how often to train/evaluate.\n",
    "* Uses AdamW optimizer, commonly effective for transformer models.\n",
    "* Lists to store training and validation losses over time.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Outer Loop: Iterate Over Epochs\n",
    "\n",
    "```python\n",
    "for iteration in range(max_iters):\n",
    "```\n",
    "\n",
    "* Runs training for `max_iters` epochs.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Inner Loop: Iterate Over Training Batches\n",
    "\n",
    "```python\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "```\n",
    "\n",
    "* Processes the training dataset batch by batch.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Periodic Evaluation\n",
    "\n",
    "```python\n",
    "if batch_idx % eval_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "    losses = estimate_loss(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "    )\n",
    "    train_losses.append(losses['train'])\n",
    "    val_losses.append(losses['val'])\n",
    "\n",
    "    print(\n",
    "        f\"iteration {iteration} / step {batch_idx}: \"\n",
    "        f\"train loss {losses['train']:.4f}, \"\n",
    "        f\"val loss {losses['val']:.4f}\"\n",
    "    )\n",
    "```\n",
    "\n",
    "* Every `eval_interval` batches (and on the last batch), the model is evaluated on full training and validation sets using the previously defined `estimate_loss` function.\n",
    "* Records and prints average losses for monitoring training progress.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Training Step\n",
    "\n",
    "```python\n",
    "logits, loss = model(x_batch, y_batch)\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "* Computes the forward pass, yielding model outputs and loss.\n",
    "* Clears previous gradients to avoid accumulation.\n",
    "* Backpropagates the loss to compute gradients.\n",
    "* Updates model weights using optimizer step.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Checkpoint Saving\n",
    "\n",
    "```python\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=iteration,\n",
    "    loss=loss.item(),\n",
    "    file_path=f\"../output/fine_tuning/run_3/checkpoint_{iteration}.pth\"\n",
    ")\n",
    "```\n",
    "\n",
    "* At the end of each epoch, saves the current state of the model and optimizer.\n",
    "* Checkpoints allow resuming training or evaluating intermediate models.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* This loop alternates between training on batches and periodically evaluating performance.\n",
    "* Evaluation helps detect issues like overfitting early.\n",
    "* Checkpointing provides safety and flexibility during long training runs.\n",
    "* Learning rate and optimizer choice are crucial hyperparameters for smooth training.\n",
    "\n",
    "---\n",
    "\n",
    "This structure represents a standard, effective fine-tuning workflow for transformer models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc13ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tokens(message: str) -> torch.Tensor:\n",
    "    input_tokens = tokenizer.encode(\n",
    "        f\"<|startoftext|>{message}<|separator|>\", allowed_special=\"all\")\n",
    "    input_tokens = torch.tensor(\n",
    "        input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "user_message = \"Salam labas\"\n",
    "input_tokens = get_input_tokens(message=user_message)\n",
    "model_answer = \"\"\n",
    "\n",
    "model.eval()\n",
    "while True:\n",
    "    output_tokens = model.generate(input_tokens=input_tokens, max_new_tokens=1)\n",
    "    last_generated_token = output_tokens[0, -1].item()\n",
    "    if last_generated_token == tokenizer.special_tokens[\"<|endoftext|>\"]:\n",
    "        break\n",
    "\n",
    "    input_tokens = torch.cat((input_tokens, output_tokens[:, -1:]), dim=1)\n",
    "    model_answer += tokenizer.decode([last_generated_token])\n",
    "\n",
    "    if len(output_tokens[0]) > block_size:\n",
    "        input_tokens = input_tokens[:, -block_size:]\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {model_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0903cbe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Explanation: Interactive Token-by-Token Text Generation Loop\n",
    "\n",
    "This code implements an **interactive generation loop**, where the model produces one token at a time in response to a user message until a special end-of-text token is generated.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Breakdown\n",
    "\n",
    "### üîπ Function to Prepare Input Tokens\n",
    "\n",
    "```python\n",
    "def get_input_tokens(message: str) -> torch.Tensor:\n",
    "    input_tokens = tokenizer.encode(\n",
    "        f\"<|startoftext|>{message}<|separator|>\", allowed_special=\"all\")\n",
    "    input_tokens = torch.tensor(\n",
    "        input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    return input_tokens\n",
    "```\n",
    "\n",
    "* Takes a raw user string and:\n",
    "\n",
    "  * Adds the special start and separator tokens to mark input boundaries.\n",
    "  * Encodes the string into token IDs using the tokenizer.\n",
    "  * Converts it into a batch tensor with shape `[1, seq_len]`.\n",
    "  * Moves it to the correct device (CPU/GPU).\n",
    "* This prepares input format expected by the GPT model.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Initialize Variables for Generation\n",
    "\n",
    "```python\n",
    "user_message = \"Salam labas\"\n",
    "input_tokens = get_input_tokens(message=user_message)\n",
    "model_answer = \"\"\n",
    "```\n",
    "\n",
    "* Sets a user prompt.\n",
    "* Encodes it into input tokens.\n",
    "* Initializes an empty string to accumulate the generated response.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Switch Model to Evaluation Mode\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "* Disables dropout and other training behaviors for consistent inference.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Token-by-Token Generation Loop\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    output_tokens = model.generate(input_tokens=input_tokens, max_new_tokens=1)\n",
    "    last_generated_token = output_tokens[0, -1].item()\n",
    "\n",
    "    if last_generated_token == tokenizer.special_tokens[\"<|endoftext|>\"]:\n",
    "        break\n",
    "\n",
    "    input_tokens = torch.cat((input_tokens, output_tokens[:, -1:]), dim=1)\n",
    "    model_answer += tokenizer.decode([last_generated_token])\n",
    "\n",
    "    if len(output_tokens[0]) > block_size:\n",
    "        input_tokens = input_tokens[:, -block_size:]\n",
    "```\n",
    "\n",
    "* Generates **one token at a time** by calling `model.generate()` with `max_new_tokens=1`.\n",
    "* Extracts the last generated token ID.\n",
    "* Stops generation if the special end-of-text token is produced.\n",
    "* Otherwise:\n",
    "\n",
    "  * Appends the newly generated token to the current input tokens for the next generation step.\n",
    "  * Decodes and accumulates the generated token into a human-readable string.\n",
    "* Keeps the input tokens length within the model's `block_size` by trimming from the left if necessary, maintaining the context window.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Print Conversation\n",
    "\n",
    "```python\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {model_answer}\")\n",
    "```\n",
    "\n",
    "* Displays the original user prompt and the model's generated response.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* This approach simulates real-time generation, token-by-token, allowing dynamic interaction.\n",
    "* Manages input context length to fit the model's constraints.\n",
    "* Continues generating until a clear stopping token signals end of response.\n",
    "* Useful for building chatbots or dialogue agents where incremental token generation is needed.\n",
    "\n",
    "---\n",
    "\n",
    "This snippet provides a practical example of how to implement fine-grained control over autoregressive text generation with your fine-tuned GPT model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
